{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL1TFdO7Wxh_"
      },
      "source": [
        "## EDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GootH4JXW5GL"
      },
      "source": [
        "#### Frequency of Reviews By Rating\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "3yG_3dddVKog",
        "outputId": "b34c4ea4-cbd7-45ec-cfa2-6cc5e166bf6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-248d05a8710d>:13: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  ax = sns.countplot(x='Score', data=train_data, palette=palette)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "train_data = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Visualize the distribution of ratings\n",
        "plt.figure(figsize=(5, 5))\n",
        "# Define a pastel palette that increases in hue from 1 to 5\n",
        "palette = sns.color_palette(\"pastel\", n_colors=5)\n",
        "\n",
        "# Plot the count plot with the specified palette\n",
        "ax = sns.countplot(x='Score', data=train_data, palette=palette)\n",
        "plt.title('Frequency of Reviews By Rating')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Get the current plot axes\n",
        "ax = plt.gca()\n",
        "\n",
        "# Loop through each patch (bar in the barplot) to get its position and height and put the text (count)\n",
        "for p in ax.patches:\n",
        "    ax.text(p.get_x() + p.get_width() / 2., p.get_height(), '%d' % int(p.get_height()),\n",
        "            fontsize=8, color='black', ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkm570-mZEUJ"
      },
      "source": [
        "#### Exploring the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfwYpSEbZZj4",
        "outputId": "03482d95-4d15-41bb-a95f-26197401b56c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Score                                               Text\n",
            "0      5  I received this product early from the seller!...\n",
            "1      5  *****<br />Numi's Collection Assortment Melang...\n",
            "2      5  I was very careful not to overcook this pasta,...\n",
            "3      5  Buying this multi-pack I was misled by the pic...\n",
            "4      5  These bars are so good! I loved them warmed up...\n"
          ]
        }
      ],
      "source": [
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyGpbaNjZhAq",
        "outputId": "84c522f4-873d-4a63-e68d-ced3887dccbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 309131 entries, 0 to 309130\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   Score   309131 non-null  int64 \n",
            " 1   Text    309131 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 4.7+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(train_data.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoYAzmR5ZlZB",
        "outputId": "10a4f282-736c-4f36-b889-b4d5e84e186e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               Score\n",
            "count  309131.000000\n",
            "mean        4.180241\n",
            "std         1.312151\n",
            "min         1.000000\n",
            "25%         4.000000\n",
            "50%         5.000000\n",
            "75%         5.000000\n",
            "max         5.000000\n"
          ]
        }
      ],
      "source": [
        "print(train_data.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxkdUqrqZshf",
        "outputId": "bbca7d10-ce81-426e-957a-30350272f067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Word Count by Rating:\n",
            "Score\n",
            "1    82.733354\n",
            "2    90.140971\n",
            "3    95.951322\n",
            "4    91.860265\n",
            "5    73.859788\n",
            "Name: word_count, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "#Average word count by Rating\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Calculate word count for each review(Text)\n",
        "train_data['word_count'] = train_data['Text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Group by 'Rating' and calculate the average word count\n",
        "average_word_count_by_rating = train_data.groupby('Score')['word_count'].mean()\n",
        "\n",
        "\n",
        "print(\"Average Word Count by Rating:\")\n",
        "print(average_word_count_by_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R_C1h6UZ3X2"
      },
      "source": [
        "### Correlation Between Word Count and Rating Of Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "0G2wgOLsZ6Xd",
        "outputId": "20bd0b1a-5ea0-4b4b-9a50-27cab1e4caec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Word Count by Rating:\n",
            "   Score  word_count\n",
            "0      1   82.733354\n",
            "1      2   90.140971\n",
            "2      3   95.951322\n",
            "3      4   91.860265\n",
            "4      5   73.859788\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-49b21710647a>:21: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  barplot = sns.barplot(x='Score', y='word_count', data=average_word_count_by_rating, palette=palette)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming train_data is already defined and loaded\n",
        "\n",
        "# Calculate word count for each review(Text)\n",
        "train_data['word_count'] = train_data['Text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Group by Rating and calculate the average word count\n",
        "average_word_count_by_rating = train_data.groupby('Score')['word_count'].mean().reset_index()\n",
        "print(\"Average Word Count by Rating:\")\n",
        "print(average_word_count_by_rating)\n",
        "\n",
        "\n",
        "# Setting up the plot\n",
        "plt.figure(figsize=(10, 6))  # Smaller graph size\n",
        "palette = sns.color_palette(\"pastel\", n_colors=len(average_word_count_by_rating['Score']))\n",
        "\n",
        "# Creating the bar plot\n",
        "barplot = sns.barplot(x='Score', y='word_count', data=average_word_count_by_rating, palette=palette)\n",
        "\n",
        "# Adding the text labels above the bars\n",
        "for p in barplot.patches:\n",
        "    barplot.annotate(format(p.get_height(), '.2f'),\n",
        "                     (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                     ha = 'center', va = 'center',\n",
        "                     size = 10,\n",
        "                     xytext = (0, 5),\n",
        "                     textcoords = 'offset points')\n",
        "\n",
        "# Setting the title and labels\n",
        "plt.title('Average Word Count by Rating', fontsize=14)\n",
        "plt.xlabel('Rating', fontsize=12)\n",
        "plt.ylabel('Average Word Count', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQgjzMwLgByM"
      },
      "source": [
        "This shows that the average word count for rating 5 is less than the average word count for rating 1 to 4, suggesting that the rating of 5 is more precise averaging approx 74 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_2nSY4Fe6Qz",
        "outputId": "4974b538-4b4e-4f9d-cf7c-51570f39df1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          count       mean        std  min   25%   50%    75%    95%     max\n",
            "Score                                                                       \n",
            "1       28521.0  82.733354  76.646718  3.0  37.0  61.0  101.0  214.0  1751.0\n",
            "2       16287.0  90.140971  79.779993  6.0  40.0  67.0  112.0  233.0  1612.0\n",
            "3       23296.0  95.951322  89.100127  7.0  41.0  70.0  121.0  255.0  3432.0\n",
            "4       43876.0  91.860265  87.705394  6.0  37.0  65.0  115.0  249.0  2061.0\n",
            "5      197151.0  73.859788  72.045997  3.0  32.0  52.0   89.0  198.0  2520.0\n"
          ]
        }
      ],
      "source": [
        "# Calculate word count for each review\n",
        "train_data['word_count'] = train_data['Text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Get distribution of word counts by rating\n",
        "distribution_by_rating = train_data.groupby('Score')['word_count'].describe(percentiles=[0.25, 0.5, 0.75, 0.95])\n",
        "\n",
        "# Print the desired statistics\n",
        "print(distribution_by_rating[['count', 'mean', 'std', 'min', '25%', '50%', '75%', '95%', 'max']])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTiXcdunyW29",
        "outputId": "aba22de6-4192-4e10-881c-f82b764cc9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxihmdMLjR1G",
        "outputId": "db938e39-530f-4aca-b1ea-026420ec95f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vs Preprocessed text:\n",
            "                                                Text  \\\n",
            "0  I received this product early from the seller!...   \n",
            "1  *****<br />Numi's Collection Assortment Melang...   \n",
            "2  I was very careful not to overcook this pasta,...   \n",
            "3  Buying this multi-pack I was misled by the pic...   \n",
            "4  These bars are so good! I loved them warmed up...   \n",
            "\n",
            "                                 Preprocessed_Review  \n",
            "0  receiv product earli seller tastey great midda...  \n",
            "1  br numi collect assort melang includesbr herba...  \n",
            "2  care overcook pasta make sure take bite everi ...  \n",
            "3  buy multipack misl pictur whole hazel nut anot...  \n",
            "4  bar good love warm definit think great snack b...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self, en_nlp=None):\n",
        "        # Download NLTK resources\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        # Initialize stemmer and stopwords\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.en_nlp = en_nlp\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return [self.preprocess_text(text) for text in X]\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove special characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "        # Remove stopwords\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        # Apply stemming\n",
        "        tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "        # Join tokens back into text\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# Create preprocessor instance\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# Apply to dataframe\n",
        "train_data['Preprocessed_Review'] = train_data['Text'].apply(preprocessor.preprocess_text)\n",
        "\n",
        "print(\"Original vs Preprocessed text:\")\n",
        "print(train_data[['Text', 'Preprocessed_Review']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5dDKwjVN10d"
      },
      "source": [
        "### Checking Anomalies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU5Mv2bYD4-s",
        "outputId": "3c232a96-d160-4a3d-cb88-37b67e5bca84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking Data Anomalies.\n",
            "Reviews with Missing Data/ Nulls: 0\n",
            "Reviews with Blanks/ Empty Spaces: 0\n",
            "Reviews with Only Stop Words: 0\n",
            "Reviews with Only Special Characters: 0\n",
            "Reviews with Only Digits: 0\n",
            "Reviews with HTML Code/ URL within text: 80712\n",
            "Reviews with only HTML Code/ URL: 0\n",
            "Reviews with containing '*': 3094\n",
            "-----------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk # no. to text coversion\n",
        "import inflect\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from bs4 import BeautifulSoup  # for HTML tag removal\n",
        "\n",
        "\n",
        "# Function to check if a review contains only stop words\n",
        "def contains_only_stopwords(text, stopwords):\n",
        "    words = re.findall(r'\\b\\w+\\b', str(text).lower())\n",
        "    return all(word in stopwords for word in words)\n",
        "\n",
        "# Function to check if a review contains only special characters\n",
        "def contains_only_special_chars(text):\n",
        "    return bool(re.match(r'^[\\W_]+$', str(text)))\n",
        "\n",
        "# Function to check if a review contains only digits\n",
        "def contains_only_digits(text):\n",
        "    return bool(re.match(r'^\\d+$', str(text)))\n",
        "\n",
        "# Function to check if a review is empty or filled with white spaces\n",
        "def is_empty_or_space(text):\n",
        "    return str(text).isspace() or not str(text)\n",
        "\n",
        "# Function to check if a review contains HTML tags or URLs\n",
        "def contains_html_or_url(text):\n",
        "    return bool(re.findall(r'<[^>]+>', str(text))) or bool(re.findall(r'http\\S+|www.\\S+', str(text)))\n",
        "\n",
        "def contains_only_html_or_url(text):\n",
        "    # Check if text contains only HTML tags\n",
        "    if bool(re.fullmatch(r'(<[^>]+>)+', str(text))):\n",
        "        return True\n",
        "    # Check if text is just a URL\n",
        "    if bool(re.fullmatch(r'http\\S+|www.\\S+', str(text))):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "stop_words_set = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Count various anomalies in the data\n",
        "missing_data_count = train_data['Text'].isnull().sum()\n",
        "blanks_count = train_data['Text'].apply(is_empty_or_space).sum()\n",
        "only_stop_words_count = train_data['Text'].apply(lambda x: contains_only_stopwords(x, stop_words_set)).sum()\n",
        "only_special_chars_count = train_data['Text'].apply(contains_only_special_chars).sum()\n",
        "only_digits_count = train_data['Text'].apply(contains_only_digits).sum()\n",
        "html_or_url_count = train_data['Text'].apply(contains_html_or_url).sum()\n",
        "only_html_or_url_count = train_data['Text'].apply(contains_only_html_or_url).sum()\n",
        "\n",
        "asterisk_reviews_count = train_data['Text'].apply(lambda x: '*' in str(x)).sum()\n",
        "\n",
        "print(\"Checking Data Anomalies.\")\n",
        "print(\"Reviews with Missing Data/ Nulls:\", missing_data_count)\n",
        "print(\"Reviews with Blanks/ Empty Spaces:\", blanks_count)\n",
        "print(\"Reviews with Only Stop Words:\", only_stop_words_count)\n",
        "print(\"Reviews with Only Special Characters:\", only_special_chars_count)\n",
        "print(\"Reviews with Only Digits:\", only_digits_count)\n",
        "print(\"Reviews with HTML Code/ URL within text:\", html_or_url_count)\n",
        "print(\"Reviews with only HTML Code/ URL:\", only_html_or_url_count)\n",
        "print(\"Reviews with containing '*':\", asterisk_reviews_count)\n",
        "print(\"-----------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIfJ3r7HOMZM"
      },
      "source": [
        "### Running Count Vectorizer and TFIDF on 6 different classifiers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYk8IBDiOOoG",
        "outputId": "5a47073f-b04c-4400-83dc-3a06438a6759"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-9b17730c3617>:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_data['Preprocessed_Review'].fillna('', inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CountVectorizer with MultinomialNB:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.58      0.57      0.58      5644\n",
            "           2       0.31      0.02      0.04      3214\n",
            "           3       0.33      0.10      0.16      4679\n",
            "           4       0.35      0.26      0.30      8688\n",
            "           5       0.76      0.93      0.84     39602\n",
            "\n",
            "    accuracy                           0.69     61827\n",
            "   macro avg       0.47      0.38      0.38     61827\n",
            "weighted avg       0.63      0.69      0.64     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 3236    76   218   307  1807]\n",
            " [  787    67   381   510  1469]\n",
            " [  536    41   487  1297  2318]\n",
            " [  353    11   224  2262  5838]\n",
            " [  659    22   167  2086 36668]]\n",
            "Using CountVectorizer with LogisticRegression:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.61      0.60      0.61      5644\n",
            "           2       0.29      0.17      0.21      3214\n",
            "           3       0.36      0.24      0.29      4679\n",
            "           4       0.40      0.21      0.28      8688\n",
            "           5       0.79      0.93      0.85     39602\n",
            "\n",
            "    accuracy                           0.71     61827\n",
            "   macro avg       0.49      0.43      0.45     61827\n",
            "weighted avg       0.66      0.71      0.67     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 3396   509   322   144  1273]\n",
            " [  879   545   552   234  1004]\n",
            " [  528   401  1127   784  1839]\n",
            " [  223   184   668  1850  5763]\n",
            " [  532   228   490  1570 36782]]\n",
            "Using CountVectorizer with DecisionTreeClassifier:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.41      0.39      0.40      5644\n",
            "           2       0.15      0.12      0.14      3214\n",
            "           3       0.19      0.17      0.18      4679\n",
            "           4       0.25      0.22      0.24      8688\n",
            "           5       0.75      0.80      0.78     39602\n",
            "\n",
            "    accuracy                           0.60     61827\n",
            "   macro avg       0.35      0.34      0.35     61827\n",
            "weighted avg       0.58      0.60      0.59     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 2200   533   525   481  1905]\n",
            " [  652   390   432   431  1309]\n",
            " [  580   356   774   890  2079]\n",
            " [  500   377   774  1936  5101]\n",
            " [ 1490   865  1504  3962 31781]]\n",
            "Using CountVectorizer with GradientBoostingClassifier:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.70      0.28      0.40      5644\n",
            "           2       0.32      0.02      0.04      3214\n",
            "           3       0.44      0.09      0.15      4679\n",
            "           4       0.48      0.11      0.18      8688\n",
            "           5       0.69      0.98      0.81     39602\n",
            "\n",
            "    accuracy                           0.68     61827\n",
            "   macro avg       0.53      0.30      0.32     61827\n",
            "weighted avg       0.62      0.68      0.60     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 1578    65    77    83  3841]\n",
            " [  335    77   194   156  2452]\n",
            " [  155    58   417   418  3631]\n",
            " [   61    15   177   978  7457]\n",
            " [  110    25    87   407 38973]]\n",
            "Using CountVectorizer with AdaBoostClassifier:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.92      0.00      0.00      5644\n",
            "           2       0.00      0.00      0.00      3214\n",
            "           3       0.00      0.00      0.00      4679\n",
            "           4       0.33      0.00      0.00      8688\n",
            "           5       0.64      1.00      0.78     39602\n",
            "\n",
            "    accuracy                           0.64     61827\n",
            "   macro avg       0.38      0.20      0.16     61827\n",
            "weighted avg       0.54      0.64      0.50     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[   11     0     0     0  5633]\n",
            " [    1     0     0     1  3212]\n",
            " [    0     0     0     1  4678]\n",
            " [    0     0     0     1  8687]\n",
            " [    0     0     0     0 39602]]\n",
            "Using CountVectorizer with KNeighborsClassifier:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.36      0.19      0.25      5644\n",
            "           2       0.17      0.04      0.07      3214\n",
            "           3       0.21      0.06      0.10      4679\n",
            "           4       0.26      0.10      0.15      8688\n",
            "           5       0.69      0.92      0.79     39602\n",
            "\n",
            "    accuracy                           0.63     61827\n",
            "   macro avg       0.34      0.26      0.27     61827\n",
            "weighted avg       0.53      0.63      0.56     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 1056   165   159   308  3956]\n",
            " [  360   135   146   240  2333]\n",
            " [  389   140   304   481  3365]\n",
            " [  325   123   330   910  7000]\n",
            " [  795   247   514  1624 36422]]\n",
            "Using TfidfVectorizer with MultinomialNB:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.01      0.02      5644\n",
            "           2       0.00      0.00      0.00      3214\n",
            "           3       0.00      0.00      0.00      4679\n",
            "           4       0.33      0.00      0.00      8688\n",
            "           5       0.64      1.00      0.78     39602\n",
            "\n",
            "    accuracy                           0.64     61827\n",
            "   macro avg       0.37      0.20      0.16     61827\n",
            "weighted avg       0.54      0.64      0.50     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[   52     0     0     0  5592]\n",
            " [    6     0     0     0  3208]\n",
            " [    1     0     0     0  4678]\n",
            " [    0     0     0     1  8687]\n",
            " [    0     1     0     2 39599]]\n",
            "Using TfidfVectorizer with LogisticRegression:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.63      0.63      0.63      5644\n",
            "           2       0.36      0.13      0.19      3214\n",
            "           3       0.39      0.23      0.29      4679\n",
            "           4       0.44      0.21      0.28      8688\n",
            "           5       0.78      0.95      0.86     39602\n",
            "\n",
            "    accuracy                           0.72     61827\n",
            "   macro avg       0.52      0.43      0.45     61827\n",
            "weighted avg       0.67      0.72      0.68     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 3578   296   270   128  1372]\n",
            " [  893   414   552   210  1145]\n",
            " [  521   277  1094   798  1989]\n",
            " [  217    98   566  1815  5992]\n",
            " [  501    81   304  1149 37567]]\n",
            "Using TfidfVectorizer with DecisionTreeClassifier:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.38      0.37      0.38      5644\n",
            "           2       0.14      0.12      0.13      3214\n",
            "           3       0.19      0.16      0.17      4679\n",
            "           4       0.24      0.22      0.23      8688\n",
            "           5       0.75      0.78      0.77     39602\n",
            "\n",
            "    accuracy                           0.59     61827\n",
            "   macro avg       0.34      0.33      0.34     61827\n",
            "weighted avg       0.57      0.59      0.58     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 2102   562   488   551  1941]\n",
            " [  664   385   453   424  1288]\n",
            " [  576   397   752   846  2108]\n",
            " [  530   406   754  1949  5049]\n",
            " [ 1646  1007  1549  4349 31051]]\n",
            "Using TfidfVectorizer with GradientBoostingClassifier:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.72      0.29      0.41      5644\n",
            "           2       0.33      0.03      0.05      3214\n",
            "           3       0.47      0.09      0.16      4679\n",
            "           4       0.49      0.11      0.18      8688\n",
            "           5       0.69      0.99      0.81     39602\n",
            "\n",
            "    accuracy                           0.68     61827\n",
            "   macro avg       0.54      0.30      0.32     61827\n",
            "weighted avg       0.63      0.68      0.60     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 1619    86    66    73  3800]\n",
            " [  334    92   188   161  2439]\n",
            " [  137    69   438   415  3620]\n",
            " [   67    11   163   984  7463]\n",
            " [  100    18    76   386 39022]]\n",
            "Using TfidfVectorizer with AdaBoostClassifier:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.79      0.00      0.01      5644\n",
            "           2       0.00      0.00      0.00      3214\n",
            "           3       0.00      0.00      0.00      4679\n",
            "           4       0.00      0.00      0.00      8688\n",
            "           5       0.64      1.00      0.78     39602\n",
            "\n",
            "    accuracy                           0.64     61827\n",
            "   macro avg       0.29      0.20      0.16     61827\n",
            "weighted avg       0.48      0.64      0.50     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[   15     0     0     0  5629]\n",
            " [    4     0     0     0  3210]\n",
            " [    0     0     0     0  4679]\n",
            " [    0     0     0     0  8688]\n",
            " [    0     0     0     0 39602]]\n",
            "Using TfidfVectorizer with KNeighborsClassifier:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.47      0.24      0.32      5644\n",
            "           2       0.16      0.04      0.07      3214\n",
            "           3       0.20      0.07      0.10      4679\n",
            "           4       0.25      0.10      0.14      8688\n",
            "           5       0.69      0.92      0.79     39602\n",
            "\n",
            "    accuracy                           0.63     61827\n",
            "   macro avg       0.35      0.27      0.28     61827\n",
            "weighted avg       0.54      0.63      0.57     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 1344   205   204   307  3584]\n",
            " [  312   138   167   255  2342]\n",
            " [  302   126   324   481  3446]\n",
            " [  264   132   339   867  7086]\n",
            " [  648   268   561  1582 36543]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB  # Added import\n",
        "from sklearn.linear_model import LogisticRegression  # Added import\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load preprocessed data\n",
        "# pre_trim_data_v3 = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Fill missing values with empty strings\n",
        "train_data['Preprocessed_Review'].fillna('', inplace=True)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = train_data['Preprocessed_Review']\n",
        "y = train_data['Score']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Define a function to train, evaluate, and compute confusion matrix\n",
        "def train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test):\n",
        "    # Vectorize the text\n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train the classifier\n",
        "    classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # Evaluate the classifier\n",
        "    y_pred = classifier.predict(X_test_vectorized)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return report, cm\n",
        "\n",
        "# Experiment with different vectorizers and classifiers\n",
        "vectorizers_v3 = [CountVectorizer(), TfidfVectorizer()]\n",
        "classifiers_v3 = [MultinomialNB(), LogisticRegression(max_iter=1000), DecisionTreeClassifier(),GradientBoostingClassifier(),AdaBoostClassifier(),KNeighborsClassifier()]\n",
        "\n",
        "for vectorizer in vectorizers_v3:\n",
        "    for classifier in classifiers_v3:\n",
        "        print(f\"Using {vectorizer.__class__.__name__} with {classifier.__class__.__name__}:\")\n",
        "        # Train and evaluate with the review data\n",
        "        report, cm = train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test)\n",
        "        print(\"Classification Report:\\n\", report)\n",
        "        print(\"Confusion Matrix:\\n\", cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table comparison:"
      ],
      "metadata": {
        "id": "XcARkQDIdHMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataframe for accuracy scores of all 6 classifiers\n",
        "df_accuracy = pd.DataFrame([\n",
        "    ['69%', '64%'],\n",
        "    ['71%', '72%'],\n",
        "    ['60%', '59%'],\n",
        "    ['68%', '68%'],\n",
        "    ['64%', '64%'],\n",
        "    ['63%', '63%']\n",
        "], index=pd.Index(['MultinomialNB', 'Logistic Regression', 'DecisionTreeClassifier', 'GradientBoostingClassifier', 'AdaBoostClassifier', 'KNeighborsClassifier']),\n",
        "   columns=pd.MultiIndex.from_product([['CountVectorizer', 'TFIDF']]))\n",
        "df_accuracy.style\n",
        "\n",
        "\n",
        "s_accuracy = df_accuracy.style\n",
        "# s_f1 = df_f1.style\n",
        "# Add table styles\n",
        "cell_hover = {'selector': 'td:hover', 'props': [('background-color', '#ffffb3')]}\n",
        "index_names = {'selector': '.index_name', 'props': 'font-style: italic; color: darkgrey; font-weight: normal;'}\n",
        "headers = {'selector': 'th:not(.index_name)', 'props': 'background-color: #000066; color: white; text-align: center'}\n",
        "s_accuracy.set_table_styles([cell_hover, index_names, headers])\n",
        "s_accuracy.set_table_styles([\n",
        "        {'selector': '.col_heading', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
        "    {'selector': '.row_heading', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
        "    {'selector': '.data', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
        "    {'selector': 'td.row1.col0',  'props': 'background-color: green; color: white'},\n",
        "    {'selector': 'td.row1.col1',  'props': 'background-color: green; color: white'},\n",
        "], overwrite=False)\n",
        "s_accuracy.set_caption(\"Comparison of classifiers performance - Accuracy\") \\\n",
        "    .set_table_styles([{\n",
        "        'selector': 'caption',\n",
        "        'props': 'caption-side: top; font-size: 20px; color: black; font-weight: bold; text-align: center; margin-bottom: 30px'\n",
        "    }], overwrite=False)\n",
        "\n",
        "# s_accuracy.format('{:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "J9RSEIoldK2i",
        "outputId": "7a28149d-9deb-47e5-dbcb-bb490f2ff09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7c93e37b1a90>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_13c32 td:hover {\n",
              "  background-color: #ffffb3;\n",
              "}\n",
              "#T_13c32 .index_name {\n",
              "  font-style: italic;\n",
              "  color: darkgrey;\n",
              "  font-weight: normal;\n",
              "}\n",
              "#T_13c32 th:not(.index_name) {\n",
              "  background-color: #000066;\n",
              "  color: white;\n",
              "  text-align: center;\n",
              "}\n",
              "#T_13c32 .col_heading {\n",
              "  font-size: 22px;\n",
              "  text-align: center;\n",
              "  border: none;\n",
              "}\n",
              "#T_13c32 .row_heading {\n",
              "  font-size: 22px;\n",
              "  text-align: center;\n",
              "  border: none;\n",
              "}\n",
              "#T_13c32 .data {\n",
              "  font-size: 22px;\n",
              "  text-align: center;\n",
              "  border: none;\n",
              "}\n",
              "#T_13c32 td.row1.col0 {\n",
              "  background-color: green;\n",
              "  color: white;\n",
              "}\n",
              "#T_13c32 td.row1.col1 {\n",
              "  background-color: green;\n",
              "  color: white;\n",
              "}\n",
              "#T_13c32 caption {\n",
              "  caption-side: top;\n",
              "  font-size: 20px;\n",
              "  color: black;\n",
              "  font-weight: bold;\n",
              "  text-align: center;\n",
              "  margin-bottom: 30px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_13c32\" class=\"dataframe\">\n",
              "  <caption>Comparison of classifiers performance - Accuracy</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_13c32_level0_col0\" class=\"col_heading level0 col0\" >('CountVectorizer',)</th>\n",
              "      <th id=\"T_13c32_level0_col1\" class=\"col_heading level0 col1\" >('TFIDF',)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_13c32_level0_row0\" class=\"row_heading level0 row0\" >MultinomialNB</th>\n",
              "      <td id=\"T_13c32_row0_col0\" class=\"data row0 col0\" >69%</td>\n",
              "      <td id=\"T_13c32_row0_col1\" class=\"data row0 col1\" >64%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_13c32_level0_row1\" class=\"row_heading level0 row1\" >Logistic Regression</th>\n",
              "      <td id=\"T_13c32_row1_col0\" class=\"data row1 col0\" >71%</td>\n",
              "      <td id=\"T_13c32_row1_col1\" class=\"data row1 col1\" >72%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_13c32_level0_row2\" class=\"row_heading level0 row2\" >DecisionTreeClassifier</th>\n",
              "      <td id=\"T_13c32_row2_col0\" class=\"data row2 col0\" >60%</td>\n",
              "      <td id=\"T_13c32_row2_col1\" class=\"data row2 col1\" >59%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_13c32_level0_row3\" class=\"row_heading level0 row3\" >GradientBoostingClassifier</th>\n",
              "      <td id=\"T_13c32_row3_col0\" class=\"data row3 col0\" >68%</td>\n",
              "      <td id=\"T_13c32_row3_col1\" class=\"data row3 col1\" >68%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_13c32_level0_row4\" class=\"row_heading level0 row4\" >AdaBoostClassifier</th>\n",
              "      <td id=\"T_13c32_row4_col0\" class=\"data row4 col0\" >64%</td>\n",
              "      <td id=\"T_13c32_row4_col1\" class=\"data row4 col1\" >64%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_13c32_level0_row5\" class=\"row_heading level0 row5\" >KNeighborsClassifier</th>\n",
              "      <td id=\"T_13c32_row5_col0\" class=\"data row5 col0\" >63%</td>\n",
              "      <td id=\"T_13c32_row5_col1\" class=\"data row5 col1\" >63%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since <b>Logistic Regression</b> performed the best, we will explore other vectorizer techniques such as <b>count vectorizer with n-grams. TFIDF with n-grams, fastText vectorizer and HashingVectorizer</b> on <b>Logistic Regression</b>."
      ],
      "metadata": {
        "id": "ZrmKJUgQhuvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizer with n-grams:\n"
      ],
      "metadata": {
        "id": "nZAvq9NHiQZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count Vectorizer on Logistic regression: (unigram, bigram, trigram):"
      ],
      "metadata": {
        "id": "uf63P8m6iU9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Fill missing values with empty strings\n",
        "train_data['Preprocessed_Review'].fillna('', inplace=True)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = train_data['Preprocessed_Review']\n",
        "y = train_data['Score']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to train, evaluate, compute confusion matrix, and save model\n",
        "def train_evaluate_confusion_matrix_save_model(vectorizer, classifier, X_train, X_test, y_train, y_test, ngram_range, model_save_path):\n",
        "    # Vectorize the text\n",
        "    vectorizer.set_params(ngram_range=ngram_range)\n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train the classifier\n",
        "    classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # Save the trained model\n",
        "    joblib.dump(classifier, model_save_path)\n",
        "\n",
        "    # Evaluate the classifier\n",
        "    y_pred = classifier.predict(X_test_vectorized)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return report, cm\n",
        "\n",
        "# Example usage with different n-gram ranges\n",
        "ngram_ranges = [(1, 1), (1, 2), (1, 3)]  # Unigram, bigram, and trigram\n",
        "model_save_paths = [\"trained_model_LR_CV_unigram.pkl\", \"trained_model_LR_CV_bigram.pkl\", \"trained_model_LR_CV_trigram.pkl\"]\n",
        "\n",
        "for ngram_range, model_save_path in zip(ngram_ranges, model_save_paths):\n",
        "    vectorizer_v9 = CountVectorizer()\n",
        "    classifier_v9 = LogisticRegression(max_iter=1000)\n",
        "\n",
        "    print(f\"Using {vectorizer_v9.__class__.__name__} with {classifier_v9.__class__.__name__} and n-gram range {ngram_range}:\")\n",
        "    #print(f\"Using {vectorizer_v9.__class.name} with {classifier_v9.__class.name_} and n-gram range {ngram_range}:\")\n",
        "    # Train, evaluate, and save the model\n",
        "    report, cm = train_evaluate_confusion_matrix_save_model(vectorizer_v9, classifier_v9, X_train, X_test, y_train, y_test, ngram_range, model_save_path)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"Model saved at:\", '/Users/Pratibha/Downloads/Checkpoint_CW2')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsB16YM5icuf",
        "outputId": "7652740e-845f-4819-ff10-939bbb882211"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-940538a038fb>:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_data['Preprocessed_Review'].fillna('', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CountVectorizer with LogisticRegression and n-gram range (1, 1):\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.61      0.60      0.61      5644\n",
            "           2       0.29      0.17      0.21      3214\n",
            "           3       0.36      0.24      0.29      4679\n",
            "           4       0.40      0.21      0.28      8688\n",
            "           5       0.79      0.93      0.85     39602\n",
            "\n",
            "    accuracy                           0.71     61827\n",
            "   macro avg       0.49      0.43      0.45     61827\n",
            "weighted avg       0.66      0.71      0.67     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 3396   509   322   144  1273]\n",
            " [  879   545   552   234  1004]\n",
            " [  528   401  1127   784  1839]\n",
            " [  223   184   668  1850  5763]\n",
            " [  532   228   490  1570 36782]]\n",
            "Model saved at: /Users/Pratibha/Downloads/Checkpoint_CW2\n",
            "\n",
            "Using CountVectorizer with LogisticRegression and n-gram range (1, 2):\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.64      0.65      0.64      5644\n",
            "           2       0.35      0.20      0.25      3214\n",
            "           3       0.41      0.28      0.34      4679\n",
            "           4       0.42      0.27      0.33      8688\n",
            "           5       0.81      0.92      0.86     39602\n",
            "\n",
            "    accuracy                           0.72     61827\n",
            "   macro avg       0.52      0.47      0.48     61827\n",
            "weighted avg       0.68      0.72      0.70     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 3655   473   315   171  1030]\n",
            " [  910   645   606   265   788]\n",
            " [  500   435  1333   892  1519]\n",
            " [  207   161   628  2362  5330]\n",
            " [  453   148   392  1993 36616]]\n",
            "Model saved at: /Users/Pratibha/Downloads/Checkpoint_CW2\n",
            "\n",
            "Using CountVectorizer with LogisticRegression and n-gram range (1, 3):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Vectorizer on Logistic regression: (unigram, bigram, trigram):"
      ],
      "metadata": {
        "id": "_yTqUC_BBHbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Fill missing values with empty strings\n",
        "train_data['Preprocessed_Review'].fillna('', inplace=True)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = train_data['Preprocessed_Review']\n",
        "y = train_data['Score']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to train, evaluate, compute confusion matrix, and save model\n",
        "def train_evaluate_confusion_matrix_save_model(vectorizer, classifier, X_train, X_test, y_train, y_test, ngram_range, model_save_path):\n",
        "    # Vectorize the text\n",
        "    vectorizer.set_params(ngram_range=ngram_range)\n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train the classifier\n",
        "    classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # Save the trained model\n",
        "    #joblib.dump(classifier, model_save_path)\n",
        "\n",
        "    # Evaluate the classifier\n",
        "    y_pred = classifier.predict(X_test_vectorized)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return report, cm\n",
        "\n",
        "# Example usage with different n-gram ranges and TF-IDF vectorization\n",
        "ngram_ranges = [(1, 1), (1, 2), (1, 3)]  # Unigram, bigram, and trigram\n",
        "model_save_paths = [\"trained_model_LR_TFIDF_unigram.pkl\", \"trained_model_LR_TFIDF_bigram.pkl\", \"trained_model_LR_TFIDF_trigram.pkl\"]\n",
        "\n",
        "for ngram_range, model_save_path in zip(ngram_ranges, model_save_paths):\n",
        "    vectorizer_v10 = TfidfVectorizer()\n",
        "    classifier_v10 = LogisticRegression(max_iter=1000)\n",
        "\n",
        "    print(f\"Using {vectorizer_v10.__class__.__name__} with {classifier_v10.__class__.__name__} and n-gram range {ngram_range}:\")\n",
        "    # Train, evaluate, and save the model\n",
        "    report, cm = train_evaluate_confusion_matrix_save_model(vectorizer_v10, classifier_v10, X_train, X_test, y_train, y_test, ngram_range, model_save_path)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    #print(\"Model saved at:\", model_save_path)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQd4HkqCDFVW",
        "outputId": "ece53d7f-af19-4549-db9e-03947a8699bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-9d6b5f10d761>:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_data['Preprocessed_Review'].fillna('', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TfidfVectorizer with LogisticRegression and n-gram range (1, 1):\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.63      0.64      0.63      5644\n",
            "           2       0.36      0.13      0.19      3214\n",
            "           3       0.39      0.24      0.30      4679\n",
            "           4       0.44      0.21      0.28      8688\n",
            "           5       0.78      0.95      0.86     39602\n",
            "\n",
            "    accuracy                           0.72     61827\n",
            "   macro avg       0.52      0.43      0.45     61827\n",
            "weighted avg       0.67      0.72      0.68     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 3584   306   270   124  1360]\n",
            " [  885   428   557   199  1145]\n",
            " [  508   280  1109   796  1986]\n",
            " [  212   101   568  1799  6008]\n",
            " [  497    90   305  1137 37573]]\n",
            "\n",
            "Using TfidfVectorizer with LogisticRegression and n-gram range (1, 2):\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.66      0.65      0.65      5644\n",
            "           2       0.42      0.12      0.19      3214\n",
            "           3       0.44      0.25      0.32      4679\n",
            "           4       0.47      0.24      0.32      8688\n",
            "           5       0.79      0.96      0.86     39602\n",
            "\n",
            "    accuracy                           0.73     61827\n",
            "   macro avg       0.56      0.44      0.47     61827\n",
            "weighted avg       0.68      0.73      0.69     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 3642   248   243   123  1388]\n",
            " [  877   398   589   248  1102]\n",
            " [  463   207  1183   895  1931]\n",
            " [  175    48   467  2080  5918]\n",
            " [  340    39   216  1090 37917]]\n",
            "\n",
            "Using TfidfVectorizer with LogisticRegression and n-gram range (1, 3):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HashingVectorizer with Logistic regression:\n"
      ],
      "metadata": {
        "id": "MZs6Wod5xqfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Fill missing values with empty strings\n",
        "train_data['Preprocessed_Review'].fillna('', inplace=True)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = train_data['Preprocessed_Review']\n",
        "y = train_data['Score']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to train, evaluate, and compute confusion matrix\n",
        "def train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test):\n",
        "    # Vectorize the text\n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train the classifier\n",
        "    classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # Evaluate the classifier\n",
        "    y_pred = classifier.predict(X_test_vectorized)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return report, cm\n",
        "\n",
        "# Experiment with different vectorizers and classifiers\n",
        "vectorizers_v2 = [HashingVectorizer()]\n",
        "classifiers_v2 = [LogisticRegression(max_iter=1000)]\n",
        "\n",
        "for vectorizer in vectorizers_v2:\n",
        "    for classifier in classifiers_v2:\n",
        "        print(f\"Using {vectorizer.__class__.__name__} with {classifier.__class__.__name__}:\")\n",
        "        # Train and evaluate with the truncated review data\n",
        "        report, cm = train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test)\n",
        "        print(\"Classification Report:\\n\", report)\n",
        "        print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecdu384txsQi",
        "outputId": "fc431725-35f3-4d8e-c070-87fd2a85a431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fe7aa8165f73>:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train_data['Preprocessed_Review'].fillna('', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using HashingVectorizer with LogisticRegression:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.63      0.64      0.63      5644\n",
            "           2       0.36      0.11      0.17      3214\n",
            "           3       0.39      0.23      0.29      4679\n",
            "           4       0.45      0.20      0.28      8688\n",
            "           5       0.78      0.95      0.86     39602\n",
            "\n",
            "    accuracy                           0.72     61827\n",
            "   macro avg       0.52      0.43      0.45     61827\n",
            "weighted avg       0.67      0.72      0.68     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 3584   260   267   116  1417]\n",
            " [  882   368   572   193  1199]\n",
            " [  513   246  1090   773  2057]\n",
            " [  226    75   557  1735  6095]\n",
            " [  504    72   280  1007 37739]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FastText Vectorizer with Logistic regression:"
      ],
      "metadata": {
        "id": "Q066APUo1HcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyqFwvI2282K",
        "outputId": "1f5385c4-6ba1-45bd-8904-eb7940d8291a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "!unzip wiki-news-300d-1M.vec.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pGuWZP43F1S",
        "outputId": "39983a6e-3d33-424c-ce0d-a31a1156e8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-28 18:35:47--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.96, 3.163.189.51, 3.163.189.108, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: wiki-news-300d-1M.vec.zip\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M   275MB/s    in 2.4s    \n",
            "\n",
            "2025-03-28 18:35:50 (275 MB/s) - wiki-news-300d-1M.vec.zip saved [681808098/681808098]\n",
            "\n",
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "class FastTextVectorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, ft_model):\n",
        "        self.ft_model = ft_model\n",
        "        self.vector_size = ft_model.vector_size\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([self.transform_text(text) for text in X])\n",
        "\n",
        "    def transform_text(self, text):\n",
        "        words = text.split()\n",
        "        vectors = [self.ft_model[word] for word in words if word in self.ft_model]\n",
        "        if vectors:\n",
        "            return np.mean(vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(self.vector_size)\n",
        "\n",
        "\n",
        "# Load the FastText model with a limit\n",
        "ft_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\")\n",
        "\n",
        "# Fill missing values with empty strings\n",
        "train_data['Preprocessed_Review'].fillna('', inplace=True)\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = train_data['Preprocessed_Review']\n",
        "y = train_data['Score']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to train, evaluate, and compute confusion matrix\n",
        "def train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test):\n",
        "    # Vectorize the text\n",
        "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train the classifier\n",
        "    classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # Evaluate the classifier\n",
        "    y_pred = classifier.predict(X_test_vectorized)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    return report, cm\n",
        "\n",
        "# Experiment with FastTextVectorizer and Logistic Regression classifier\n",
        "vectorizer = FastTextVectorizer(ft_model)\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "\n",
        "print(f\"Using FastTextVectorizer with {classifier.__class__.__name__}:\")\n",
        "# Train and evaluate with the truncated review data\n",
        "report, cm = train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test)\n",
        "print(\"Classification Report:\\n\", report)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svPGtAV51NMb",
        "outputId": "28d9ebf3-c0c4-4b2f-cd1e-fdcc19b1267e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using FastTextVectorizer with LogisticRegression:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.53      0.43      0.47      5644\n",
            "           2       0.33      0.03      0.05      3214\n",
            "           3       0.32      0.09      0.14      4679\n",
            "           4       0.38      0.06      0.10      8688\n",
            "           5       0.70      0.96      0.81     39602\n",
            "\n",
            "    accuracy                           0.67     61827\n",
            "   macro avg       0.45      0.31      0.32     61827\n",
            "weighted avg       0.59      0.67      0.59     61827\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 2399    64   149    54  2978]\n",
            " [  657    95   260    77  2125]\n",
            " [  468    62   429   286  3434]\n",
            " [  287    27   278   512  7584]\n",
            " [  728    36   215   429 38194]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataframe for accuracy scores of two vectorizers\n",
        "df_accuracy = pd.DataFrame([\n",
        "    ['72%', '67%'],\n",
        "], index=pd.Index(['Logistic Regression']),\n",
        "   columns=pd.MultiIndex.from_product([['HashingVectorizer', 'FastText Vectorizer']]))\n",
        "df_accuracy.style\n",
        "\n",
        "\n",
        "s_accuracy = df_accuracy.style\n",
        "#s_f1 = df_f1.style\n",
        "# Add table styles\n",
        "cell_hover = {'selector': 'td:hover', 'props': [('background-color', '#ffffb3')]}\n",
        "index_names = {'selector': '.index_name', 'props': 'font-style: italic; color: darkgrey; font-weight: normal;'}\n",
        "headers = {'selector': 'th:not(.index_name)', 'props': 'background-color: #000066; color: white; text-align: center'}\n",
        "s_accuracy.set_table_styles([cell_hover, index_names, headers])\n",
        "s_accuracy.set_table_styles([\n",
        "        {'selector': '.col_heading', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
        "    {'selector': '.row_heading', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
        "    {'selector': '.data', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
        "    {'selector': 'td.row0.col0',  'props': 'background-color: green; color: white'},\n",
        "#     {'selector': 'td.row1.col1',  'props': 'background-color: green; color: white'},\n",
        "], overwrite=False)\n",
        "s_accuracy.set_caption(\"Comparison of HashingVectorizer and FastText Vectorizer using Logistic Regression\") \\\n",
        "    .set_table_styles([{\n",
        "        'selector': 'caption',\n",
        "        'props': 'caption-side: top; font-size: 20px; color: black; font-weight: bold; text-align: center; margin-bottom: 30px'\n",
        "    }], overwrite=False)\n",
        "\n",
        "# s_accuracy.format('{:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "hhWi58gE_yLm",
        "outputId": "97adeb2f-6d24-43eb-8d3f-8d96701c7004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7e44b58d3c90>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_f3232 td:hover {\n",
              "  background-color: #ffffb3;\n",
              "}\n",
              "#T_f3232 .index_name {\n",
              "  font-style: italic;\n",
              "  color: darkgrey;\n",
              "  font-weight: normal;\n",
              "}\n",
              "#T_f3232 th:not(.index_name) {\n",
              "  background-color: #000066;\n",
              "  color: white;\n",
              "  text-align: center;\n",
              "}\n",
              "#T_f3232 .col_heading {\n",
              "  font-size: 22px;\n",
              "  text-align: center;\n",
              "  border: none;\n",
              "}\n",
              "#T_f3232 .row_heading {\n",
              "  font-size: 22px;\n",
              "  text-align: center;\n",
              "  border: none;\n",
              "}\n",
              "#T_f3232 .data {\n",
              "  font-size: 22px;\n",
              "  text-align: center;\n",
              "  border: none;\n",
              "}\n",
              "#T_f3232 td.row0.col0 {\n",
              "  background-color: green;\n",
              "  color: white;\n",
              "}\n",
              "#T_f3232 caption {\n",
              "  caption-side: top;\n",
              "  font-size: 20px;\n",
              "  color: black;\n",
              "  font-weight: bold;\n",
              "  text-align: center;\n",
              "  margin-bottom: 30px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_f3232\" class=\"dataframe\">\n",
              "  <caption>Comparison of HashingVectorizer and FastText Vectorizer using Logistic Regression</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_f3232_level0_col0\" class=\"col_heading level0 col0\" >('HashingVectorizer',)</th>\n",
              "      <th id=\"T_f3232_level0_col1\" class=\"col_heading level0 col1\" >('FastText Vectorizer',)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_f3232_level0_row0\" class=\"row_heading level0 row0\" >Logistic Regression</th>\n",
              "      <td id=\"T_f3232_row0_col0\" class=\"data row0 col0\" >72%</td>\n",
              "      <td id=\"T_f3232_row0_col1\" class=\"data row0 col1\" >67%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Space Model for sequential models:"
      ],
      "metadata": {
        "id": "bZXPR8diu8PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec model:"
      ],
      "metadata": {
        "id": "zmTbn_8Cu-Zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creates Word2Vec using only minimal processing - CBOW:"
      ],
      "metadata": {
        "id": "MwRRyHimvD0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "\n",
        "import logging\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# Load preprocessed data\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Extracting features (X) and labels (y)\n",
        "X = data['Text'].astype(str)\n",
        "y = data['Score']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n",
        "             ):\n",
        "    '''\n",
        "    Convert a raw review to a cleaned review\n",
        "    '''\n",
        "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
        "    words = letters_only.lower().split() # convert to lower case\n",
        "\n",
        "    if remove_stopwords: # remove stopword\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    if stemming==True: # stemming\n",
        "#         stemmer = PorterStemmer()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        words = [stemmer.stem(w) for w in words]\n",
        "\n",
        "    if split_text==True:  # split text\n",
        "        return (words)\n",
        "\n",
        "    return( \" \".join(words))\n",
        "\n",
        "X_train_cleaned = []\n",
        "X_test_cleaned = []\n",
        "\n",
        "for d in X_train:\n",
        "    X_train_cleaned.append(cleanText(d))\n",
        "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
        "\n",
        "for d in X_test:\n",
        "    X_test_cleaned.append(cleanText(d))\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def parseSent(review, tokenizer, remove_stopwords=False):\n",
        "    '''\n",
        "    Parse text into sentences\n",
        "    '''\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# Parse each review in the training set into sentences\n",
        "sentences = []\n",
        "for review in X_train_cleaned:\n",
        "    sentences += parseSent(review, tokenizer)\n",
        "\n",
        "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
        "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZTGLKPDvIln",
        "outputId": "30d9b4ad-c373-4b54-cde5-92c85d6b8719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d1635ef6c106>:31: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
            "\n",
            "    filehandle = open(your filename)\n",
            "\n",
            "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Show a cleaned review in the training set : \n",
            " we were house sitting for our neighbors the past couple of weeks and found this item that we thought might come in handy especially with their pups or dog and pup don t know the age limit for canines anyway chef michael s grilled sirloin literally drove them wild their owners feed them purina dog chow but once we opened the bag and pour a bowl full for each they dove right in and went scurrying to the bag when we put it down something that didn t happen with purina when they came back our neighbor ricardo commented on how much the dogs seems to prefer the chef michael s now i don t know what they re paying for the purina or if the chef michael s would be a treat here and there but one thing is certain the chef michael s grilled sirloin made quite an impression on those dogs they really really love the taste of this stuff\n",
            "247304 parsed sentence in the training set\n",
            "\n",
            "Show a parsed sentence in the training set : \n",
            " ['we', 'were', 'house', 'sitting', 'for', 'our', 'neighbors', 'the', 'past', 'couple', 'of', 'weeks', 'and', 'found', 'this', 'item', 'that', 'we', 'thought', 'might', 'come', 'in', 'handy', 'especially', 'with', 'their', 'pups', 'or', 'dog', 'and', 'pup', 'don', 't', 'know', 'the', 'age', 'limit', 'for', 'canines', 'anyway', 'chef', 'michael', 's', 'grilled', 'sirloin', 'literally', 'drove', 'them', 'wild', 'their', 'owners', 'feed', 'them', 'purina', 'dog', 'chow', 'but', 'once', 'we', 'opened', 'the', 'bag', 'and', 'pour', 'a', 'bowl', 'full', 'for', 'each', 'they', 'dove', 'right', 'in', 'and', 'went', 'scurrying', 'to', 'the', 'bag', 'when', 'we', 'put', 'it', 'down', 'something', 'that', 'didn', 't', 'happen', 'with', 'purina', 'when', 'they', 'came', 'back', 'our', 'neighbor', 'ricardo', 'commented', 'on', 'how', 'much', 'the', 'dogs', 'seems', 'to', 'prefer', 'the', 'chef', 'michael', 's', 'now', 'i', 'don', 't', 'know', 'what', 'they', 're', 'paying', 'for', 'the', 'purina', 'or', 'if', 'the', 'chef', 'michael', 's', 'would', 'be', 'a', 'treat', 'here', 'and', 'there', 'but', 'one', 'thing', 'is', 'certain', 'the', 'chef', 'michael', 's', 'grilled', 'sirloin', 'made', 'quite', 'an', 'impression', 'on', 'those', 'dogs', 'they', 'really', 'really', 'love', 'the', 'taste', 'of', 'this', 'stuff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit parsed sentences to Word2Vec model\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
        "\n",
        "num_features = 300  #embedding dimension\n",
        "min_word_count = 10\n",
        "num_workers = 4\n",
        "context = 10\n",
        "downsampling = 1e-3\n",
        "\n",
        "print(\"Training Word2Vec model ...\\n\")\n",
        "w2v = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count,\n",
        "                 window = context, sample = downsampling)\n",
        "w2v.init_sims(replace=True)\n",
        "w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\n",
        "\n",
        "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index_to_key)) #4016\n",
        "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index_to_key[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-3Z0zDR03Pz",
        "outputId": "071284fc-177d-41ed-e5cf-c46a880e3654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Word2Vec model ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ca7b7e26e529>:13: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  w2v.init_sims(replace=True)\n",
            "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the vocabulary list : 20599 \n",
            "\n",
            "Show first 10 words in the vocalbulary list  vocabulary list: \n",
            " ['the', 'i', 'and', 'a', 'it', 'to', 'of', 'is', 'this', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creates Word2Vec model using minimal preprocessing - SkipGram:\n"
      ],
      "metadata": {
        "id": "GqRFllHk3bLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "\n",
        "import logging\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# Load preprocessed data\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Extracting features (X) and labels (y)\n",
        "X = data['Text'].astype(str)\n",
        "y = data['Score']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def cleanText(raw_text, remove_stopwords=False, stemming=False, lemmatization = False, split_text=False, \\\n",
        "             ):\n",
        "    '''\n",
        "    Convert a raw review to a cleaned review\n",
        "    '''\n",
        "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
        "    words = letters_only.lower().split() # convert to lower case\n",
        "\n",
        "    if remove_stopwords: # remove stopword\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    if stemming==True: # stemming\n",
        "#         stemmer = PorterStemmer()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        words = [stemmer.stem(w) for w in words]\n",
        "\n",
        "    if lemmatization:  # Lemmatization\n",
        "        lemmatized_tokens = []\n",
        "        for token in en_nlp(\" \".join(words)):\n",
        "            if token.is_alpha and not token.is_stop:\n",
        "                pos_tag = token.pos_\n",
        "                wn_pos_tag = get_wordnet_pos(pos_tag)\n",
        "                if wn_pos_tag is not None:\n",
        "                    lemmatized_tokens.append(token.lemma_)\n",
        "                else:\n",
        "                    lemmatized_tokens.append(token.text)\n",
        "        words = lemmatized_tokens\n",
        "\n",
        "    if split_text==True:  # split text\n",
        "        return (words)\n",
        "\n",
        "    return( \" \".join(words))\n",
        "\n",
        "def get_wordnet_pos(pos):\n",
        "        if pos.startswith('ADJ'):\n",
        "            return 'a'  # Adjective\n",
        "        elif pos.startswith('ADV'):\n",
        "            return 'r'  # Adverb\n",
        "        elif pos.startswith('NOUN'):\n",
        "            return 'n'  # Noun\n",
        "        elif pos.startswith('VERB'):\n",
        "            return 'v'  # Verb\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "X_train_cleaned = []\n",
        "X_test_cleaned = []\n",
        "\n",
        "for d in X_train:\n",
        "    X_train_cleaned.append(cleanText(d))\n",
        "\n",
        "for d in X_test:\n",
        "    X_test_cleaned.append(cleanText(d))\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def parseSent(review, tokenizer, remove_stopwords=False):\n",
        "    '''\n",
        "    Parse text into sentences\n",
        "    '''\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# Parse each review in the training set into sentences\n",
        "sentences = []\n",
        "for review in X_train_cleaned:\n",
        "    sentences += parseSent(review, tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b76pM6zo3kSr",
        "outputId": "91f06f6b-1a3e-40d3-c4a2-9f82990cac73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-84020faeb0e6>:31: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
            "\n",
            "    filehandle = open(your filename)\n",
            "\n",
            "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creates Word2Vec using minimal and Stemming processing - CBOW:"
      ],
      "metadata": {
        "id": "KrjVpMqs4vOl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e57943b",
        "outputId": "26675320-caed-477f-eaab-4c518d4b6f3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-0a1b00fb6d0b>:32: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
            "\n",
            "    filehandle = open(your filename)\n",
            "\n",
            "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Show a cleaned review in the training set : \n",
            " we were house sitting for our neighbors the past couple of weeks and found this item that we thought might come in handy especially with their pups or dog and pup don t know the age limit for canines anyway chef michael s grilled sirloin literally drove them wild their owners feed them purina dog chow but once we opened the bag and pour a bowl full for each they dove right in and went scurrying to the bag when we put it down something that didn t happen with purina when they came back our neighbor ricardo commented on how much the dogs seems to prefer the chef michael s now i don t know what they re paying for the purina or if the chef michael s would be a treat here and there but one thing is certain the chef michael s grilled sirloin made quite an impression on those dogs they really really love the taste of this stuff\n",
            "Training Word2Vec model ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-0a1b00fb6d0b>:114: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  w2v.init_sims(replace=True)\n",
            "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the vocabulary list : 13981 \n",
            "\n",
            "Show first 10 words in the vocalbulary list  vocabulary list: \n",
            " ['the', 'i', 'and', 'a', 'it', 'to', 'of', 'is', 'this', 'in']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "import spacy\n",
        "\n",
        "import logging\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load preprocessed data\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Extracting features (X) and labels (y)\n",
        "X = data['Text'].astype(str)\n",
        "y = data['Score']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load spaCy English language model\n",
        "en_nlp = spacy.load(\"en_core_web_sm\")\n",
        "def cleanText(raw_text, remove_stopwords=False, stemming=False, lemmatization = False, split_text=False, \\\n",
        "             ):\n",
        "    '''\n",
        "    Convert a raw review to a cleaned review\n",
        "    '''\n",
        "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
        "    words = letters_only.lower().split() # convert to lower case\n",
        "\n",
        "    if remove_stopwords: # remove stopword\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    if stemming==True: # stemming\n",
        "#         stemmer = PorterStemmer()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        words = [stemmer.stem(w) for w in words]\n",
        "\n",
        "    if lemmatization:  # Lemmatization\n",
        "        lemmatized_tokens = []\n",
        "        for token in en_nlp(\" \".join(words)):\n",
        "            if token.is_alpha and not token.is_stop:\n",
        "                pos_tag = token.pos_\n",
        "                wn_pos_tag = get_wordnet_pos(pos_tag)\n",
        "                if wn_pos_tag is not None:\n",
        "                    lemmatized_tokens.append(token.lemma_)\n",
        "                else:\n",
        "                    lemmatized_tokens.append(token.text)\n",
        "        words = lemmatized_tokens\n",
        "\n",
        "    if split_text==True:  # split text\n",
        "        return (words)\n",
        "\n",
        "    return( \" \".join(words))\n",
        "\n",
        "def get_wordnet_pos(pos):\n",
        "        if pos.startswith('ADJ'):\n",
        "            return 'a'  # Adjective\n",
        "        elif pos.startswith('ADV'):\n",
        "            return 'r'  # Adverb\n",
        "        elif pos.startswith('NOUN'):\n",
        "            return 'n'  # Noun\n",
        "        elif pos.startswith('VERB'):\n",
        "            return 'v'  # Verb\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "def parseSent(review, tokenizer, remove_stopwords=False):\n",
        "    '''\n",
        "    Parse text into sentences\n",
        "    '''\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(cleanText(raw_sentence, remove_stopwords=False, stemming=True, lemmatization = False, split_text=True))\n",
        "    return sentences\n",
        "\n",
        "X_train_cleaned = []\n",
        "X_test_cleaned = []\n",
        "\n",
        "for d in X_train:\n",
        "    X_train_cleaned.append(cleanText(d))\n",
        "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
        "\n",
        "# for d in X_test:\n",
        "#     X_test_cleaned.append(cleanText(d))\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Parse each review in the training set into sentences\n",
        "sentences = []\n",
        "for review in X_train_cleaned:\n",
        "    sentences += parseSent(review, tokenizer)\n",
        "\n",
        "# sentences = []\n",
        "# for review in X_test_cleaned:\n",
        "#     sentences += parseSent(review, tokenizer)\n",
        "num_features = 300  #embedding dimension\n",
        "min_word_count = 10\n",
        "num_workers = 4\n",
        "context = 10\n",
        "downsampling = 1e-3\n",
        "\n",
        "print(\"Training Word2Vec model ...\\n\")\n",
        "w2v = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count,\n",
        "                 window = context, sample = downsampling)\n",
        "w2v.init_sims(replace=True)\n",
        "w2v.save(\"w2v_model\") #save trained word2vec model\n",
        "\n",
        "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index_to_key)) #4016\n",
        "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index_to_key[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creates Word2Vec model using minimal and stemming preprocessing - SkipGram:"
      ],
      "metadata": {
        "id": "LT5oCHTMdxA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "import spacy\n",
        "\n",
        "# Load preprocessed data\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Extracting features (X) and labels (y)\n",
        "X = data['Text'].astype(str)\n",
        "y = data['Score']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load spaCy English language model\n",
        "en_nlp = spacy.load(\"en_core_web_sm\")\n",
        "def cleanText(raw_text, remove_stopwords=False, stemming=False, lemmatization = False, split_text=False, \\\n",
        "             ):\n",
        "    '''\n",
        "    Convert a raw review to a cleaned review\n",
        "    '''\n",
        "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
        "    words = letters_only.lower().split() # convert to lower case\n",
        "\n",
        "    if remove_stopwords: # remove stopword\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    if stemming==True: # stemming\n",
        "#         stemmer = PorterStemmer()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        words = [stemmer.stem(w) for w in words]\n",
        "\n",
        "    if lemmatization:  # Lemmatization\n",
        "        lemmatized_tokens = []\n",
        "        for token in en_nlp(\" \".join(words)):\n",
        "            if token.is_alpha and not token.is_stop:\n",
        "                pos_tag = token.pos_\n",
        "                wn_pos_tag = get_wordnet_pos(pos_tag)\n",
        "                if wn_pos_tag is not None:\n",
        "                    lemmatized_tokens.append(token.lemma_)\n",
        "                else:\n",
        "                    lemmatized_tokens.append(token.text)\n",
        "        words = lemmatized_tokens\n",
        "\n",
        "    if split_text==True:  # split text\n",
        "        return (words)\n",
        "\n",
        "    return( \" \".join(words))\n",
        "\n",
        "def get_wordnet_pos(pos):\n",
        "        if pos.startswith('ADJ'):\n",
        "            return 'a'  # Adjective\n",
        "        elif pos.startswith('ADV'):\n",
        "            return 'r'  # Adverb\n",
        "        elif pos.startswith('NOUN'):\n",
        "            return 'n'  # Noun\n",
        "        elif pos.startswith('VERB'):\n",
        "            return 'v'  # Verb\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "def parseSent(review, tokenizer, remove_stopwords=False):\n",
        "    '''\n",
        "    Parse text into sentences\n",
        "    '''\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(cleanText(raw_sentence, remove_stopwords=False, stemming=True, lemmatization = False, split_text=True))\n",
        "    return sentences\n",
        "\n",
        "X_train_cleaned = []\n",
        "X_test_cleaned = []\n",
        "\n",
        "for d in X_train:\n",
        "    X_train_cleaned.append(cleanText(d))\n",
        "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
        "\n",
        "# for d in X_test:\n",
        "#     X_test_cleaned.append(cleanText(d))\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Parse each review in the training set into sentences\n",
        "sentences = []\n",
        "for review in X_train_cleaned:\n",
        "    sentences += parseSent(review, tokenizer)\n",
        "\n",
        "# sentences = []\n",
        "# for review in X_test_cleaned:\n",
        "#     sentences += parseSent(review, tokenizer)\n",
        "num_features = 300  #embedding dimension\n",
        "min_word_count = 10\n",
        "num_workers = 4\n",
        "context = 10\n",
        "downsampling = 1e-3\n",
        "\n",
        "print(\"Training Word2Vec model ...\\n\")\n",
        "w2v = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count,\n",
        "                 window = context, sample = downsampling, sg=1)\n",
        "w2v.init_sims(replace=True)\n",
        "w2v.save(\"word2vec_models_SG\") #save trained word2vec model\n",
        "\n",
        "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index_to_key)) #4016\n",
        "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index_to_key[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "sBSZ1y7XeJNW",
        "outputId": "a36c7ab5-1117-4980-9a42-d55c31c0cbb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-c54a98a2f67a>:22: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
            "\n",
            "    filehandle = open(your filename)\n",
            "\n",
            "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Show a cleaned review in the training set : \n",
            " we were house sitting for our neighbors the past couple of weeks and found this item that we thought might come in handy especially with their pups or dog and pup don t know the age limit for canines anyway chef michael s grilled sirloin literally drove them wild their owners feed them purina dog chow but once we opened the bag and pour a bowl full for each they dove right in and went scurrying to the bag when we put it down something that didn t happen with purina when they came back our neighbor ricardo commented on how much the dogs seems to prefer the chef michael s now i don t know what they re paying for the purina or if the chef michael s would be a treat here and there but one thing is certain the chef michael s grilled sirloin made quite an impression on those dogs they really really love the taste of this stuff\n",
            "Training Word2Vec model ...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c54a98a2f67a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Word2Vec model ...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m w2v = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count,\n\u001b[0m\u001b[1;32m    103\u001b[0m                  window = context, sample = downsampling, sg=1)\n\u001b[1;32m    104\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             self.train(\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[1;32m   1074\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[1;32m   1435\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creates Word2Vec using minimal and Lemmatizer processing - CBOW:\n"
      ],
      "metadata": {
        "id": "L8fevH4tgVjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English language model\n",
        "en_nlp = spacy.load(\"en_core_web_sm\")\n",
        "import logging\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# Load preprocessed data\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Extracting features (X) and labels (y)\n",
        "X = data['Text'].astype(str)\n",
        "y = data['Score']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def cleanText(raw_text, remove_stopwords=False, lemmatization=False, split_text=False, \\\n",
        "             ):\n",
        "    '''\n",
        "    Convert a raw review to a cleaned review\n",
        "    '''\n",
        "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
        "    words = letters_only.lower().split() # convert to lower case\n",
        "\n",
        "    if remove_stopwords: # remove stopword\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "#     if stemming==True: # stemming\n",
        "# #         stemmer = PorterStemmer()\n",
        "#         stemmer = SnowballStemmer('english')\n",
        "#         words = [stemmer.stem(w) for w in words]\n",
        "\n",
        "    if lemmatization:  # Lemmatization\n",
        "        lemmatized_tokens = []\n",
        "        for token in en_nlp(\" \".join(words)):\n",
        "            if token.is_alpha and not token.is_stop:\n",
        "                pos_tag = token.pos_\n",
        "                wn_pos_tag = get_wordnet_pos(pos_tag)\n",
        "                if wn_pos_tag is not None:\n",
        "                    lemmatized_tokens.append(token.lemma_)\n",
        "                else:\n",
        "                    lemmatized_tokens.append(token.text)\n",
        "        words = lemmatized_tokens\n",
        "    if split_text==True:  # split text\n",
        "        return (words)\n",
        "\n",
        "    return( \" \".join(words))\n",
        "\n",
        "def get_wordnet_pos(pos):\n",
        "        if pos.startswith('ADJ'):\n",
        "            return 'a'  # Adjective\n",
        "        elif pos.startswith('ADV'):\n",
        "            return 'r'  # Adverb\n",
        "        elif pos.startswith('NOUN'):\n",
        "            return 'n'  # Noun\n",
        "        elif pos.startswith('VERB'):\n",
        "            return 'v'  # Verb\n",
        "        else:\n",
        "            return None\n",
        "X_train_cleaned = []\n",
        "X_test_cleaned = []\n",
        "\n",
        "for d in X_train:\n",
        "    X_train_cleaned.append(cleanText(d))\n",
        "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
        "\n",
        "for d in X_test:\n",
        "    X_test_cleaned.append(cleanText(d))\n",
        "\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def parseSent(review, tokenizer, remove_stopwords=False):\n",
        "    '''\n",
        "    Parse text into sentences\n",
        "    '''\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(cleanText(raw_sentence, remove_stopwords, lemmatization=True, split_text=True))\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# Parse each review in the training set into sentences\n",
        "sentences = []\n",
        "for review in X_train_cleaned:\n",
        "    sentences += parseSent(review, tokenizer)\n",
        "\n",
        "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
        "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "NpVxFvNigpge",
        "outputId": "145a845d-ded8-4f7f-e992-ac06808dfed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-2a7dc87ee2a9>:34: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
            "\n",
            "    filehandle = open(your filename)\n",
            "\n",
            "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Show a cleaned review in the training set : \n",
            " we were house sitting for our neighbors the past couple of weeks and found this item that we thought might come in handy especially with their pups or dog and pup don t know the age limit for canines anyway chef michael s grilled sirloin literally drove them wild their owners feed them purina dog chow but once we opened the bag and pour a bowl full for each they dove right in and went scurrying to the bag when we put it down something that didn t happen with purina when they came back our neighbor ricardo commented on how much the dogs seems to prefer the chef michael s now i don t know what they re paying for the purina or if the chef michael s would be a treat here and there but one thing is certain the chef michael s grilled sirloin made quite an impression on those dogs they really really love the taste of this stuff\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2a7dc87ee2a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train_cleaned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparseSent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%d parsed sentence in the training set\\n'\u001b[0m  \u001b[0;34m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2a7dc87ee2a9>\u001b[0m in \u001b[0;36mparseSent\u001b[0;34m(review, tokenizer, remove_stopwords)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mraw_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_sentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2a7dc87ee2a9>\u001b[0m in \u001b[0;36mcleanText\u001b[0;34m(raw_text, remove_stopwords, lemmatization, split_text)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Lemmatization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mlemmatized_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0men_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alpha\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mpos_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/pipeline/trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.set_annotations\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/pipeline/_parser_internals/ner.pyx\u001b[0m in \u001b[0;36mspacy.pipeline._parser_internals.ner.BiluoPushDown.set_annotations\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.SetEntsDefault.values\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/enum.py\u001b[0m in \u001b[0;36m__members__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_member_names_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mbltns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__members__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part - 4: Model training, selection and hyperparameter tuning and evaluation:"
      ],
      "metadata": {
        "id": "iGxsgTcsAIwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "from joblib import dump\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import os\n",
        "\n",
        "# Create directory for saving models\n",
        "os.makedirs(\"./grid_search_models\", exist_ok=True)\n",
        "\n",
        "\n",
        "# Fill missing values with empty strings\n",
        "train_data['Preprocessed_Review'] = train_data['Preprocessed_Review'].fillna('')\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = train_data['Preprocessed_Review']\n",
        "y = train_data['Score']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a list of classifiers with their associated parameters\n",
        "classifiers = [\n",
        "    {\n",
        "        'classifier': [MultinomialNB()],\n",
        "        'classifier__alpha': [0.1, 0.5, 1.0],  # Smoothing parameter for MultinomialNB\n",
        "    },\n",
        "    {\n",
        "        'classifier': [LogisticRegression(max_iter=1000)],\n",
        "        'classifier__C': [0.01, 0.1, 1.0, 10.0],  # Regularization parameter for LogisticRegression\n",
        "        'classifier__penalty': ['l1', 'l2'],  # Penalty term for LogisticRegression\n",
        "    },\n",
        "    {\n",
        "        'classifier': [GradientBoostingClassifier()],\n",
        "        'classifier__n_estimators': [50, 100, 200],  # The number of boosting stages to perform\n",
        "        'classifier__max_depth': [3, 5, 7],  # Maximum depth of the individual trees\n",
        "    },\n",
        "]\n",
        "max_df_values = [0.2, 0.5, 1.0]\n",
        "vectorizers = []\n",
        "for max_df in max_df_values:\n",
        "    vectorizers.extend([\n",
        "        (CountVectorizer(ngram_range=(1, 1), max_df=max_df), f\"CountVectorizer_Unigram_max_df_{max_df}\"),\n",
        "        (CountVectorizer(ngram_range=(1, 2), max_df=max_df), f\"CountVectorizer_Bigram_max_df_{max_df}\"),\n",
        "        (CountVectorizer(ngram_range=(1, 3), max_df=max_df), f\"CountVectorizer_Trigram_max_df_{max_df}\"),\n",
        "        (TfidfVectorizer(ngram_range=(1, 1), max_df=max_df), f\"TfidfVectorizer_Unigram_max_df_{max_df}\"),\n",
        "        (TfidfVectorizer(ngram_range=(1, 2), max_df=max_df), f\"TfidfVectorizer_Bigram_max_df_{max_df}\"),\n",
        "        (TfidfVectorizer(ngram_range=(1, 3), max_df=max_df), f\"TfidfVectorizer_Trigram_max_df_{max_df}\"),\n",
        "    ])\n",
        "\n",
        "# Create an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# Perform grid search for each combination of classifier and vectorizer\n",
        "for classifier_config in classifiers:\n",
        "    for vectorizer, vectorizer_name in vectorizers:\n",
        "        print(\"Classifier:\", type(classifier_config['classifier'][0]).__name__)\n",
        "        print(\"vectorizer\", vectorizer_name)\n",
        "        # Create a pipeline with the current vectorizer and classifier\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('classifier', classifier_config['classifier']),\n",
        "        ])\n",
        "\n",
        "        # Create GridSearchCV object\n",
        "        grid_search = GridSearchCV(pipeline, classifier_config, cv=2, scoring='accuracy', verbose=3)\n",
        "\n",
        "        # Perform grid search\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Save the best model\n",
        "        best_model = grid_search.best_estimator_\n",
        "        model_filename = f\"./grid_search_models/{vectorizer_name}_{type(classifier_config['classifier'][0]).__name__}_best_model.joblib\"\n",
        "        dump(best_model, model_filename)\n",
        "\n",
        "        # Get best parameters and best score\n",
        "        best_params = grid_search.best_params_\n",
        "        best_score = grid_search.best_score_\n",
        "        test_score = grid_search.score(X_test, y_test)\n",
        "\n",
        "        # Store the results\n",
        "        result = {\n",
        "            'vectorizer': vectorizer_name,\n",
        "            'classifier': type(classifier_config['classifier'][0]).__name__,\n",
        "            'best_params': best_params,\n",
        "            'best_score': best_score,\n",
        "            'test_score': test_score,\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "\n",
        "# Print the results\n",
        "for result in results:\n",
        "    print(\"===================================================\")\n",
        "    print(f\"Vectorizer: {result['vectorizer']}, Classifier: {result['classifier']}\")\n",
        "    print(\"Best parameters:\", result['best_params'])\n",
        "    print(\"Best cross-validation score:\", result['best_score'])\n",
        "    print(\"Test set accuracy:\", result['test_score'])\n",
        "    print()\n",
        "\n",
        "    # Print classification report and confusion matrix\n",
        "    print(\"Classification Report:\")\n",
        "    y_pred = result['best_model'].predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"===================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rErw7BvuALZD",
        "outputId": "f051e3cc-04bd-46d4-f221-86812ce57bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier: MultinomialNB\n",
            "vectorizer CountVectorizer_Unigram_max_df_0.2\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.663 total time=   8.4s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.663 total time=   7.4s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.680 total time=   7.8s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=   8.3s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.683 total time=   6.8s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.684 total time=   8.4s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer CountVectorizer_Bigram_max_df_0.2\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.690 total time=  25.5s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.690 total time=  25.8s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.680 total time=  26.1s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=  26.1s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.653 total time=  26.3s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.653 total time=  26.3s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer CountVectorizer_Trigram_max_df_0.2\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.692 total time=  55.0s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.691 total time=  54.9s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.656 total time=  55.3s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.656 total time=  56.9s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.643 total time=  55.2s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.643 total time=  55.1s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer TfidfVectorizer_Unigram_max_df_0.2\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.658 total time=   8.6s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.657 total time=   7.2s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   8.7s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   7.1s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   8.6s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   8.7s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer TfidfVectorizer_Bigram_max_df_0.2\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.648 total time=  26.1s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.647 total time=  26.2s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  25.5s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  26.0s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  26.1s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  26.1s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer TfidfVectorizer_Trigram_max_df_0.2\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  55.9s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  57.0s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  55.3s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  55.5s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  55.5s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  56.4s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer CountVectorizer_Unigram_max_df_0.5\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.667 total time=   7.9s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.665 total time=   8.4s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.683 total time=   6.8s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.682 total time=   8.2s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.686 total time=   6.9s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.686 total time=   8.3s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer CountVectorizer_Bigram_max_df_0.5\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.692 total time=  26.1s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.691 total time=  26.1s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=  26.3s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=  26.1s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.652 total time=  26.3s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.651 total time=  26.1s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer CountVectorizer_Trigram_max_df_0.5\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.693 total time=  55.0s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.692 total time=  56.5s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.655 total time=  55.2s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.654 total time=  55.2s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.642 total time=  54.9s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.643 total time=  56.0s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer TfidfVectorizer_Unigram_max_df_0.5\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.658 total time=   6.7s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.658 total time=   8.4s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   7.8s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   7.3s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   8.3s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   6.8s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer TfidfVectorizer_Bigram_max_df_0.5\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.648 total time=  25.9s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.647 total time=  26.1s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  26.0s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  26.1s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  26.5s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  26.6s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer TfidfVectorizer_Trigram_max_df_0.5\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  56.0s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  55.6s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  56.2s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  55.9s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  55.9s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  56.3s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer CountVectorizer_Unigram_max_df_1.0\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.667 total time=   7.3s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.665 total time=   8.4s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.683 total time=   6.8s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.682 total time=   8.3s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.686 total time=   6.7s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.686 total time=   8.3s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer CountVectorizer_Bigram_max_df_1.0\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.692 total time=  26.2s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.691 total time=  26.0s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=  26.3s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=  26.5s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.652 total time=  25.7s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.651 total time=  25.7s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer CountVectorizer_Trigram_max_df_1.0\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.693 total time=  54.6s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.692 total time=  55.2s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.655 total time=  56.0s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.654 total time=  55.1s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.642 total time=  54.7s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.643 total time=  54.9s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer TfidfVectorizer_Unigram_max_df_1.0\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.658 total time=   8.0s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.658 total time=   8.4s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   6.9s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   8.2s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   6.9s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   8.4s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer TfidfVectorizer_Bigram_max_df_1.0\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.648 total time=  26.3s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.647 total time=  26.4s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  26.6s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  26.3s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  25.9s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  25.6s\n",
            "Classifier: MultinomialNB\n",
            "vectorizer TfidfVectorizer_Trigram_max_df_1.0\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  56.6s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  55.8s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  55.8s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  55.9s\n",
            "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  56.6s\n",
            "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  56.1s\n",
            "Classifier: LogisticRegression\n",
            "vectorizer CountVectorizer_Unigram_max_df_0.2\n",
            "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=   4.7s\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=   3.6s\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.698 total time=  21.8s\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.699 total time=  23.7s\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=   3.5s\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=   4.9s\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.703 total time=  43.3s\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.703 total time=  45.8s\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=   3.4s\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=   5.0s\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.692 total time= 1.5min\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.692 total time= 1.5min\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=   5.0s\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=   3.4s\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.665 total time= 2.6min\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.664 total time= 2.8min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "8 fits failed out of a total of 16.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "8 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 662, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.69824588        nan 0.70279494        nan 0.69190551\n",
            "        nan 0.66454243]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier: LogisticRegression\n",
            "vectorizer CountVectorizer_Bigram_max_df_0.2\n",
            "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=  15.2s\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=  15.5s\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.709 total time= 2.6min\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.709 total time= 2.7min\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=  15.1s\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=  15.7s\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.719 total time= 4.7min\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.719 total time= 4.9min\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=  15.0s\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=  15.4s\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.713 total time= 5.0min\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.714 total time= 5.9min\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=  15.5s\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=  15.6s\n",
            "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.707 total time= 4.4min\n",
            "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.708 total time= 4.5min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "8 fits failed out of a total of 16.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "8 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 662, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.70878757        nan 0.71888445        nan 0.71304548\n",
            "        nan 0.70767962]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}