{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL1TFdO7Wxh_"
   },
   "source": [
    "## EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GootH4JXW5GL"
   },
   "source": [
    "#### Frequency of Reviews By Rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.3\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "GPU List:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU List: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "3yG_3dddVKog",
    "outputId": "b34c4ea4-cbd7-45ec-cfa2-6cc5e166bf6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\AppData\\Local\\Temp\\ipykernel_4608\\2265954618.py:13: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.countplot(x='Score', data=train_data, palette=palette)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Visualize the distribution of ratings\n",
    "plt.figure(figsize=(5, 5))\n",
    "# Define a pastel palette that increases in hue from 1 to 5\n",
    "palette = sns.color_palette(\"pastel\", n_colors=5)\n",
    "\n",
    "# Plot the count plot with the specified palette\n",
    "ax = sns.countplot(x='Score', data=train_data, palette=palette)\n",
    "plt.title('Frequency of Reviews By Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Get the current plot axes\n",
    "ax = plt.gca()\n",
    "\n",
    "# Loop through each patch (bar in the barplot) to get its position and height and put the text (count)\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width() / 2., p.get_height(), '%d' % int(p.get_height()),\n",
    "            fontsize=8, color='black', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kkm570-mZEUJ"
   },
   "source": [
    "#### Exploring the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfwYpSEbZZj4",
    "outputId": "03482d95-4d15-41bb-a95f-26197401b56c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Score                                               Text\n",
      "0      5  I received this product early from the seller!...\n",
      "1      5  *****<br />Numi's Collection Assortment Melang...\n",
      "2      5  I was very careful not to overcook this pasta,...\n",
      "3      5  Buying this multi-pack I was misled by the pic...\n",
      "4      5  These bars are so good! I loved them warmed up...\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyGpbaNjZhAq",
    "outputId": "84c522f4-873d-4a63-e68d-ced3887dccbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 309131 entries, 0 to 309130\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Score   309131 non-null  int64 \n",
      " 1   Text    309131 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SoYAzmR5ZlZB",
    "outputId": "10a4f282-736c-4f36-b889-b4d5e84e186e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Score\n",
      "count  309131.000000\n",
      "mean        4.180241\n",
      "std         1.312151\n",
      "min         1.000000\n",
      "25%         4.000000\n",
      "50%         5.000000\n",
      "75%         5.000000\n",
      "max         5.000000\n"
     ]
    }
   ],
   "source": [
    "print(train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxkdUqrqZshf",
    "outputId": "bbca7d10-ce81-426e-957a-30350272f067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Count by Rating:\n",
      "Score\n",
      "1    82.733354\n",
      "2    90.140971\n",
      "3    95.951322\n",
      "4    91.860265\n",
      "5    73.859788\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Average word count by Rating\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Calculate word count for each review(Text)\n",
    "train_data['word_count'] = train_data['Text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Group by 'Rating' and calculate the average word count\n",
    "average_word_count_by_rating = train_data.groupby('Score')['word_count'].mean()\n",
    "\n",
    "\n",
    "print(\"Average Word Count by Rating:\")\n",
    "print(average_word_count_by_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R_C1h6UZ3X2"
   },
   "source": [
    "### Correlation Between Word Count and Rating Of Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "id": "0G2wgOLsZ6Xd",
    "outputId": "20bd0b1a-5ea0-4b4b-9a50-27cab1e4caec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Count by Rating:\n",
      "   Score  word_count\n",
      "0      1   82.733354\n",
      "1      2   90.140971\n",
      "2      3   95.951322\n",
      "3      4   91.860265\n",
      "4      5   73.859788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\AppData\\Local\\Temp\\ipykernel_12528\\2455535619.py:21: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  barplot = sns.barplot(x='Score', y='word_count', data=average_word_count_by_rating, palette=palette)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming train_data is already defined and loaded\n",
    "\n",
    "# Calculate word count for each review(Text)\n",
    "train_data['word_count'] = train_data['Text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Group by Rating and calculate the average word count\n",
    "average_word_count_by_rating = train_data.groupby('Score')['word_count'].mean().reset_index()\n",
    "print(\"Average Word Count by Rating:\")\n",
    "print(average_word_count_by_rating)\n",
    "\n",
    "\n",
    "# Setting up the plot\n",
    "plt.figure(figsize=(10, 6))  # Smaller graph size\n",
    "palette = sns.color_palette(\"pastel\", n_colors=len(average_word_count_by_rating['Score']))\n",
    "\n",
    "# Creating the bar plot\n",
    "barplot = sns.barplot(x='Score', y='word_count', data=average_word_count_by_rating, palette=palette)\n",
    "\n",
    "# Adding the text labels above the bars\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.2f'),\n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                     ha = 'center', va = 'center',\n",
    "                     size = 10,\n",
    "                     xytext = (0, 5),\n",
    "                     textcoords = 'offset points')\n",
    "\n",
    "# Setting the title and labels\n",
    "plt.title('Average Word Count by Rating', fontsize=14)\n",
    "plt.xlabel('Rating', fontsize=12)\n",
    "plt.ylabel('Average Word Count', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQgjzMwLgByM"
   },
   "source": [
    "This shows that the average word count for rating 5 is less than the average word count for rating 1 to 4, suggesting that the rating of 5 is more precise averaging approx 74 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_2nSY4Fe6Qz",
    "outputId": "4974b538-4b4e-4f9d-cf7c-51570f39df1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count       mean        std  min   25%   50%    75%    95%     max\n",
      "Score                                                                       \n",
      "1       28521.0  82.733354  76.646718  3.0  37.0  61.0  101.0  214.0  1751.0\n",
      "2       16287.0  90.140971  79.779993  6.0  40.0  67.0  112.0  233.0  1612.0\n",
      "3       23296.0  95.951322  89.100127  7.0  41.0  70.0  121.0  255.0  3432.0\n",
      "4       43876.0  91.860265  87.705394  6.0  37.0  65.0  115.0  249.0  2061.0\n",
      "5      197151.0  73.859788  72.045997  3.0  32.0  52.0   89.0  198.0  2520.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate word count for each review\n",
    "train_data['word_count'] = train_data['Text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Get distribution of word counts by rating\n",
    "distribution_by_rating = train_data.groupby('Score')['word_count'].describe(percentiles=[0.25, 0.5, 0.75, 0.95])\n",
    "\n",
    "# Print the desired statistics\n",
    "print(distribution_by_rating[['count', 'mean', 'std', 'min', '25%', '50%', '75%', '95%', 'max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTiXcdunyW29",
    "outputId": "aba22de6-4192-4e10-881c-f82b764cc9d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\rahar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Preprocessed text:\n",
      "                                                Text  \\\n",
      "0  I received this product early from the seller!...   \n",
      "1  *****<br />Numi's Collection Assortment Melang...   \n",
      "2  I was very careful not to overcook this pasta,...   \n",
      "3  Buying this multi-pack I was misled by the pic...   \n",
      "4  These bars are so good! I loved them warmed up...   \n",
      "\n",
      "                                 Preprocessed_Review  \n",
      "0  receiv product earli seller tastey great midda...  \n",
      "1  br numi collect assort melang includesbr herba...  \n",
      "2  care overcook pasta make sure take bite everi ...  \n",
      "3  buy multipack misl pictur whole hazel nut anot...  \n",
      "4  bar good love warm definit think great snack b...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, en_nlp=None):\n",
    "        # Initialize stemmer and stopwords\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.en_nlp = en_nlp\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.preprocess_text(text) for text in X]\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        # Apply stemming\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        # Join tokens back into text\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Create preprocessor instance\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Apply to dataframe\n",
    "train_data['Preprocessed_Review'] = train_data['Text'].apply(preprocessor.preprocess_text)\n",
    "\n",
    "print(\"Original vs Preprocessed text:\")\n",
    "print(train_data[['Text', 'Preprocessed_Review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5dDKwjVN10d"
   },
   "source": [
    "### Checking Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XU5Mv2bYD4-s",
    "outputId": "3c232a96-d160-4a3d-cb88-37b67e5bca84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Data Anomalies.\n",
      "Reviews with Missing Data/ Nulls: 0\n",
      "Reviews with Blanks/ Empty Spaces: 0\n",
      "Reviews with Only Stop Words: 0\n",
      "Reviews with Only Special Characters: 0\n",
      "Reviews with Only Digits: 0\n",
      "Reviews with HTML Code/ URL within text: 80712\n",
      "Reviews with only HTML Code/ URL: 0\n",
      "Reviews with containing '*': 3094\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk # no. to text coversion\n",
    "import inflect\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from bs4 import BeautifulSoup  # for HTML tag removal\n",
    "\n",
    "\n",
    "# Function to check if a review contains only stop words\n",
    "def contains_only_stopwords(text, stopwords):\n",
    "    words = re.findall(r'\\b\\w+\\b', str(text).lower())\n",
    "    return all(word in stopwords for word in words)\n",
    "\n",
    "# Function to check if a review contains only special characters\n",
    "def contains_only_special_chars(text):\n",
    "    return bool(re.match(r'^[\\W_]+$', str(text)))\n",
    "\n",
    "# Function to check if a review contains only digits\n",
    "def contains_only_digits(text):\n",
    "    return bool(re.match(r'^\\d+$', str(text)))\n",
    "\n",
    "# Function to check if a review is empty or filled with white spaces\n",
    "def is_empty_or_space(text):\n",
    "    return str(text).isspace() or not str(text)\n",
    "\n",
    "# Function to check if a review contains HTML tags or URLs\n",
    "def contains_html_or_url(text):\n",
    "    return bool(re.findall(r'<[^>]+>', str(text))) or bool(re.findall(r'http\\S+|www.\\S+', str(text)))\n",
    "\n",
    "def contains_only_html_or_url(text):\n",
    "    # Check if text contains only HTML tags\n",
    "    if bool(re.fullmatch(r'(<[^>]+>)+', str(text))):\n",
    "        return True\n",
    "    # Check if text is just a URL\n",
    "    if bool(re.fullmatch(r'http\\S+|www.\\S+', str(text))):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "stop_words_set = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Count various anomalies in the data\n",
    "missing_data_count = train_data['Text'].isnull().sum()\n",
    "blanks_count = train_data['Text'].apply(is_empty_or_space).sum()\n",
    "only_stop_words_count = train_data['Text'].apply(lambda x: contains_only_stopwords(x, stop_words_set)).sum()\n",
    "only_special_chars_count = train_data['Text'].apply(contains_only_special_chars).sum()\n",
    "only_digits_count = train_data['Text'].apply(contains_only_digits).sum()\n",
    "html_or_url_count = train_data['Text'].apply(contains_html_or_url).sum()\n",
    "only_html_or_url_count = train_data['Text'].apply(contains_only_html_or_url).sum()\n",
    "\n",
    "asterisk_reviews_count = train_data['Text'].apply(lambda x: '*' in str(x)).sum()\n",
    "\n",
    "print(\"Checking Data Anomalies.\")\n",
    "print(\"Reviews with Missing Data/ Nulls:\", missing_data_count)\n",
    "print(\"Reviews with Blanks/ Empty Spaces:\", blanks_count)\n",
    "print(\"Reviews with Only Stop Words:\", only_stop_words_count)\n",
    "print(\"Reviews with Only Special Characters:\", only_special_chars_count)\n",
    "print(\"Reviews with Only Digits:\", only_digits_count)\n",
    "print(\"Reviews with HTML Code/ URL within text:\", html_or_url_count)\n",
    "print(\"Reviews with only HTML Code/ URL:\", only_html_or_url_count)\n",
    "print(\"Reviews with containing '*':\", asterisk_reviews_count)\n",
    "print(\"-----------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIfJ3r7HOMZM"
   },
   "source": [
    "### Running Count Vectorizer and TFIDF on 6 different classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.3\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYk8IBDiOOoG",
    "outputId": "5a47073f-b04c-4400-83dc-3a06438a6759"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\AppData\\Local\\Temp\\ipykernel_12528\\1673106063.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['Preprocessed_Review'].fillna('', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CountVectorizer with MultinomialNB:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.57      0.58      5644\n",
      "           2       0.31      0.02      0.04      3214\n",
      "           3       0.33      0.10      0.16      4679\n",
      "           4       0.35      0.26      0.30      8688\n",
      "           5       0.76      0.93      0.84     39602\n",
      "\n",
      "    accuracy                           0.69     61827\n",
      "   macro avg       0.47      0.38      0.38     61827\n",
      "weighted avg       0.63      0.69      0.64     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3236    76   218   307  1807]\n",
      " [  787    67   381   510  1469]\n",
      " [  536    41   487  1297  2318]\n",
      " [  353    11   224  2262  5838]\n",
      " [  659    22   167  2086 36668]]\n",
      "Using CountVectorizer with LogisticRegression:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.60      0.61      5644\n",
      "           2       0.29      0.17      0.21      3214\n",
      "           3       0.36      0.24      0.29      4679\n",
      "           4       0.40      0.21      0.28      8688\n",
      "           5       0.79      0.93      0.85     39602\n",
      "\n",
      "    accuracy                           0.71     61827\n",
      "   macro avg       0.49      0.43      0.45     61827\n",
      "weighted avg       0.66      0.71      0.67     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3395   511   322   142  1274]\n",
      " [  879   544   551   231  1009]\n",
      " [  523   402  1132   777  1845]\n",
      " [  220   184   669  1846  5769]\n",
      " [  532   227   483  1563 36797]]\n",
      "Using CountVectorizer with DecisionTreeClassifier:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.39      0.40      5644\n",
      "           2       0.15      0.12      0.13      3214\n",
      "           3       0.20      0.17      0.18      4679\n",
      "           4       0.25      0.22      0.24      8688\n",
      "           5       0.75      0.80      0.78     39602\n",
      "\n",
      "    accuracy                           0.60     61827\n",
      "   macro avg       0.35      0.34      0.34     61827\n",
      "weighted avg       0.58      0.60      0.59     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 2198   548   510   503  1885]\n",
      " [  665   376   429   434  1310]\n",
      " [  569   370   799   895  2046]\n",
      " [  499   379   752  1923  5135]\n",
      " [ 1511   860  1536  3912 31783]]\n",
      "Using CountVectorizer with GradientBoostingClassifier:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.28      0.40      5644\n",
      "           2       0.32      0.02      0.04      3214\n",
      "           3       0.44      0.09      0.15      4679\n",
      "           4       0.48      0.11      0.18      8688\n",
      "           5       0.69      0.98      0.81     39602\n",
      "\n",
      "    accuracy                           0.68     61827\n",
      "   macro avg       0.53      0.30      0.32     61827\n",
      "weighted avg       0.62      0.68      0.60     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 1577    66    77    83  3841]\n",
      " [  335    77   194   156  2452]\n",
      " [  155    58   417   418  3631]\n",
      " [   61    15   177   978  7457]\n",
      " [  110    22    87   407 38976]]\n",
      "Using CountVectorizer with AdaBoostClassifier:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\Cuda_gpu_env_tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rahar\\.conda\\envs\\Cuda_gpu_env_tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rahar\\.conda\\envs\\Cuda_gpu_env_tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.00      0.00      5644\n",
      "           2       0.00      0.00      0.00      3214\n",
      "           3       0.00      0.00      0.00      4679\n",
      "           4       0.33      0.00      0.00      8688\n",
      "           5       0.64      1.00      0.78     39602\n",
      "\n",
      "    accuracy                           0.64     61827\n",
      "   macro avg       0.38      0.20      0.16     61827\n",
      "weighted avg       0.54      0.64      0.50     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   11     0     0     0  5633]\n",
      " [    1     0     0     1  3212]\n",
      " [    0     0     0     1  4678]\n",
      " [    0     0     0     1  8687]\n",
      " [    0     0     0     0 39602]]\n",
      "Using CountVectorizer with KNeighborsClassifier:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.36      0.19      0.25      5644\n",
      "           2       0.17      0.04      0.07      3214\n",
      "           3       0.21      0.06      0.10      4679\n",
      "           4       0.26      0.10      0.15      8688\n",
      "           5       0.69      0.92      0.79     39602\n",
      "\n",
      "    accuracy                           0.63     61827\n",
      "   macro avg       0.34      0.26      0.27     61827\n",
      "weighted avg       0.53      0.63      0.56     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 1056   165   159   308  3956]\n",
      " [  360   135   146   240  2333]\n",
      " [  389   140   304   481  3365]\n",
      " [  325   123   330   910  7000]\n",
      " [  795   247   514  1624 36422]]\n",
      "Using TfidfVectorizer with MultinomialNB:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\Cuda_gpu_env_tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rahar\\.conda\\envs\\Cuda_gpu_env_tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rahar\\.conda\\envs\\Cuda_gpu_env_tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.01      0.02      5644\n",
      "           2       0.00      0.00      0.00      3214\n",
      "           3       0.00      0.00      0.00      4679\n",
      "           4       0.33      0.00      0.00      8688\n",
      "           5       0.64      1.00      0.78     39602\n",
      "\n",
      "    accuracy                           0.64     61827\n",
      "   macro avg       0.37      0.20      0.16     61827\n",
      "weighted avg       0.54      0.64      0.50     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   52     0     0     0  5592]\n",
      " [    6     0     0     0  3208]\n",
      " [    1     0     0     0  4678]\n",
      " [    0     0     0     1  8687]\n",
      " [    0     1     0     2 39599]]\n",
      "Using TfidfVectorizer with LogisticRegression:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.64      0.63      5644\n",
      "           2       0.35      0.13      0.19      3214\n",
      "           3       0.39      0.24      0.29      4679\n",
      "           4       0.44      0.21      0.28      8688\n",
      "           5       0.78      0.95      0.86     39602\n",
      "\n",
      "    accuracy                           0.72     61827\n",
      "   macro avg       0.52      0.43      0.45     61827\n",
      "weighted avg       0.67      0.72      0.68     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3586   301   267   129  1361]\n",
      " [  890   413   557   208  1146]\n",
      " [  519   282  1102   790  1986]\n",
      " [  216   100   566  1797  6009]\n",
      " [  496    80   308  1129 37589]]\n",
      "Using TfidfVectorizer with DecisionTreeClassifier:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.38      0.37      0.37      5644\n",
      "           2       0.14      0.12      0.13      3214\n",
      "           3       0.19      0.16      0.17      4679\n",
      "           4       0.24      0.22      0.23      8688\n",
      "           5       0.75      0.78      0.77     39602\n",
      "\n",
      "    accuracy                           0.58     61827\n",
      "   macro avg       0.34      0.33      0.33     61827\n",
      "weighted avg       0.57      0.58      0.58     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 2094   583   505   540  1922]\n",
      " [  679   378   425   444  1288]\n",
      " [  584   404   745   857  2089]\n",
      " [  493   422   783  1923  5067]\n",
      " [ 1715   969  1565  4337 31016]]\n",
      "Using TfidfVectorizer with GradientBoostingClassifier:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.29      0.41      5644\n",
      "           2       0.33      0.03      0.05      3214\n",
      "           3       0.47      0.09      0.16      4679\n",
      "           4       0.49      0.11      0.18      8688\n",
      "           5       0.69      0.99      0.81     39602\n",
      "\n",
      "    accuracy                           0.68     61827\n",
      "   macro avg       0.54      0.30      0.32     61827\n",
      "weighted avg       0.63      0.68      0.60     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 1619    86    66    73  3800]\n",
      " [  334    92   188   161  2439]\n",
      " [  137    69   438   415  3620]\n",
      " [   67    11   163   982  7465]\n",
      " [  100    17    76   386 39023]]\n",
      "Using TfidfVectorizer with AdaBoostClassifier:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\Cuda_gpu_env_tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rahar\\.conda\\envs\\Cuda_gpu_env_tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\rahar\\.conda\\envs\\Cuda_gpu_env_tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.00      0.01      5644\n",
      "           2       0.00      0.00      0.00      3214\n",
      "           3       0.00      0.00      0.00      4679\n",
      "           4       0.00      0.00      0.00      8688\n",
      "           5       0.64      1.00      0.78     39602\n",
      "\n",
      "    accuracy                           0.64     61827\n",
      "   macro avg       0.29      0.20      0.16     61827\n",
      "weighted avg       0.48      0.64      0.50     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[   15     0     0     0  5629]\n",
      " [    4     0     0     0  3210]\n",
      " [    0     0     0     0  4679]\n",
      " [    0     0     0     0  8688]\n",
      " [    0     0     0     0 39602]]\n",
      "Using TfidfVectorizer with KNeighborsClassifier:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.24      0.32      5644\n",
      "           2       0.16      0.04      0.07      3214\n",
      "           3       0.20      0.07      0.10      4679\n",
      "           4       0.25      0.10      0.14      8688\n",
      "           5       0.69      0.92      0.79     39602\n",
      "\n",
      "    accuracy                           0.63     61827\n",
      "   macro avg       0.35      0.27      0.28     61827\n",
      "weighted avg       0.54      0.63      0.57     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 1344   205   204   307  3584]\n",
      " [  312   138   167   255  2342]\n",
      " [  302   126   324   481  3446]\n",
      " [  264   132   339   867  7086]\n",
      " [  648   268   561  1582 36543]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB  # Added import\n",
    "from sklearn.linear_model import LogisticRegression  # Added import\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load preprocessed data\n",
    "# pre_trim_data_v3 = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "train_data['Preprocessed_Review'].fillna('', inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = train_data['Preprocessed_Review']\n",
    "y = train_data['Score']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define a function to train, evaluate, and compute confusion matrix\n",
    "def train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test):\n",
    "    # Vectorize the text\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = classifier.predict(X_test_vectorized)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return report, cm\n",
    "\n",
    "# Experiment with different vectorizers and classifiers\n",
    "vectorizers_v3 = [CountVectorizer(), TfidfVectorizer()]\n",
    "classifiers_v3 = [MultinomialNB(), LogisticRegression(max_iter=1000), DecisionTreeClassifier(),GradientBoostingClassifier(),AdaBoostClassifier(),KNeighborsClassifier()]\n",
    "\n",
    "for vectorizer in vectorizers_v3:\n",
    "    for classifier in classifiers_v3:\n",
    "        print(f\"Using {vectorizer.__class__.__name__} with {classifier.__class__.__name__}:\")\n",
    "        # Train and evaluate with the review data\n",
    "        report, cm = train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test)\n",
    "        print(\"Classification Report:\\n\", report)\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcARkQDIdHMt"
   },
   "source": [
    "### Table comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "J9RSEIoldK2i",
    "outputId": "7a28149d-9deb-47e5-dbcb-bb490f2ff09c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2073c td:hover {\n",
       "  background-color: #ffffb3;\n",
       "}\n",
       "#T_2073c .index_name {\n",
       "  font-style: italic;\n",
       "  color: darkgrey;\n",
       "  font-weight: normal;\n",
       "}\n",
       "#T_2073c th:not(.index_name) {\n",
       "  background-color: #000066;\n",
       "  color: white;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_2073c .col_heading {\n",
       "  font-size: 22px;\n",
       "  text-align: center;\n",
       "  border: none;\n",
       "}\n",
       "#T_2073c .row_heading {\n",
       "  font-size: 22px;\n",
       "  text-align: center;\n",
       "  border: none;\n",
       "}\n",
       "#T_2073c .data {\n",
       "  font-size: 22px;\n",
       "  text-align: center;\n",
       "  border: none;\n",
       "}\n",
       "#T_2073c td.row1.col0 {\n",
       "  background-color: green;\n",
       "  color: white;\n",
       "}\n",
       "#T_2073c td.row1.col1 {\n",
       "  background-color: green;\n",
       "  color: white;\n",
       "}\n",
       "#T_2073c caption {\n",
       "  caption-side: top;\n",
       "  font-size: 20px;\n",
       "  color: black;\n",
       "  font-weight: bold;\n",
       "  text-align: center;\n",
       "  margin-bottom: 30px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2073c\">\n",
       "  <caption>Comparison of classifiers performance - Accuracy</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2073c_level0_col0\" class=\"col_heading level0 col0\" >('CountVectorizer',)</th>\n",
       "      <th id=\"T_2073c_level0_col1\" class=\"col_heading level0 col1\" >('TFIDF',)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2073c_level0_row0\" class=\"row_heading level0 row0\" >MultinomialNB</th>\n",
       "      <td id=\"T_2073c_row0_col0\" class=\"data row0 col0\" >69%</td>\n",
       "      <td id=\"T_2073c_row0_col1\" class=\"data row0 col1\" >64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2073c_level0_row1\" class=\"row_heading level0 row1\" >Logistic Regression</th>\n",
       "      <td id=\"T_2073c_row1_col0\" class=\"data row1 col0\" >71%</td>\n",
       "      <td id=\"T_2073c_row1_col1\" class=\"data row1 col1\" >72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2073c_level0_row2\" class=\"row_heading level0 row2\" >DecisionTreeClassifier</th>\n",
       "      <td id=\"T_2073c_row2_col0\" class=\"data row2 col0\" >60%</td>\n",
       "      <td id=\"T_2073c_row2_col1\" class=\"data row2 col1\" >59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2073c_level0_row3\" class=\"row_heading level0 row3\" >GradientBoostingClassifier</th>\n",
       "      <td id=\"T_2073c_row3_col0\" class=\"data row3 col0\" >68%</td>\n",
       "      <td id=\"T_2073c_row3_col1\" class=\"data row3 col1\" >68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2073c_level0_row4\" class=\"row_heading level0 row4\" >AdaBoostClassifier</th>\n",
       "      <td id=\"T_2073c_row4_col0\" class=\"data row4 col0\" >64%</td>\n",
       "      <td id=\"T_2073c_row4_col1\" class=\"data row4 col1\" >64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2073c_level0_row5\" class=\"row_heading level0 row5\" >KNeighborsClassifier</th>\n",
       "      <td id=\"T_2073c_row5_col0\" class=\"data row5 col0\" >63%</td>\n",
       "      <td id=\"T_2073c_row5_col1\" class=\"data row5 col1\" >63%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2873a4aae00>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe for accuracy scores of all 6 classifiers\n",
    "df_accuracy = pd.DataFrame([\n",
    "    ['69%', '64%'],\n",
    "    ['71%', '72%'],\n",
    "    ['60%', '59%'],\n",
    "    ['68%', '68%'],\n",
    "    ['64%', '64%'],\n",
    "    ['63%', '63%']\n",
    "], index=pd.Index(['MultinomialNB', 'Logistic Regression', 'DecisionTreeClassifier', 'GradientBoostingClassifier', 'AdaBoostClassifier', 'KNeighborsClassifier']),\n",
    "   columns=pd.MultiIndex.from_product([['CountVectorizer', 'TFIDF']]))\n",
    "df_accuracy.style\n",
    "\n",
    "\n",
    "s_accuracy = df_accuracy.style\n",
    "# s_f1 = df_f1.style\n",
    "# Add table styles\n",
    "cell_hover = {'selector': 'td:hover', 'props': [('background-color', '#ffffb3')]}\n",
    "index_names = {'selector': '.index_name', 'props': 'font-style: italic; color: darkgrey; font-weight: normal;'}\n",
    "headers = {'selector': 'th:not(.index_name)', 'props': 'background-color: #000066; color: white; text-align: center'}\n",
    "s_accuracy.set_table_styles([cell_hover, index_names, headers])\n",
    "s_accuracy.set_table_styles([\n",
    "        {'selector': '.col_heading', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
    "    {'selector': '.row_heading', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
    "    {'selector': '.data', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
    "    {'selector': 'td.row1.col0',  'props': 'background-color: green; color: white'},\n",
    "    {'selector': 'td.row1.col1',  'props': 'background-color: green; color: white'},\n",
    "], overwrite=False)\n",
    "s_accuracy.set_caption(\"Comparison of classifiers performance - Accuracy\") \\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'caption',\n",
    "        'props': 'caption-side: top; font-size: 20px; color: black; font-weight: bold; text-align: center; margin-bottom: 30px'\n",
    "    }], overwrite=False)\n",
    "\n",
    "# s_accuracy.format('{:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrmKJUgQhuvl"
   },
   "source": [
    "Since <b>Logistic Regression</b> performed the best, we will explore other vectorizer techniques such as <b>count vectorizer with n-grams. TFIDF with n-grams, fastText vectorizer and HashingVectorizer</b> on <b>Logistic Regression</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZAvq9NHiQZ4"
   },
   "source": [
    "## Vectorizer with n-grams:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf63P8m6iU9E"
   },
   "source": [
    "## Count Vectorizer on Logistic regression: (unigram, bigram, trigram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsB16YM5icuf",
    "outputId": "7652740e-845f-4819-ff10-939bbb882211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CountVectorizer with LogisticRegression and n-gram range (1, 1):\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.60      0.61      5644\n",
      "           2       0.29      0.17      0.21      3214\n",
      "           3       0.36      0.24      0.29      4679\n",
      "           4       0.40      0.21      0.28      8688\n",
      "           5       0.79      0.93      0.85     39602\n",
      "\n",
      "    accuracy                           0.71     61827\n",
      "   macro avg       0.49      0.43      0.45     61827\n",
      "weighted avg       0.66      0.71      0.67     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3395   511   322   142  1274]\n",
      " [  879   544   551   231  1009]\n",
      " [  523   402  1132   777  1845]\n",
      " [  220   184   669  1846  5769]\n",
      " [  532   227   483  1563 36797]]\n",
      "Model saved at: /Users/Pratibha/Downloads/Checkpoint_CW2\n",
      "\n",
      "Using CountVectorizer with LogisticRegression and n-gram range (1, 2):\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.65      0.64      5644\n",
      "           2       0.35      0.20      0.26      3214\n",
      "           3       0.41      0.29      0.34      4679\n",
      "           4       0.42      0.27      0.33      8688\n",
      "           5       0.81      0.93      0.86     39602\n",
      "\n",
      "    accuracy                           0.72     61827\n",
      "   macro avg       0.52      0.47      0.49     61827\n",
      "weighted avg       0.68      0.72      0.70     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3676   467   315   164  1022]\n",
      " [  917   652   607   255   783]\n",
      " [  503   439  1342   897  1498]\n",
      " [  209   165   637  2348  5329]\n",
      " [  454   155   388  1972 36633]]\n",
      "Model saved at: /Users/Pratibha/Downloads/Checkpoint_CW2\n",
      "\n",
      "Using CountVectorizer with LogisticRegression and n-gram range (1, 3):\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.66      0.65      5644\n",
      "           2       0.36      0.18      0.24      3214\n",
      "           3       0.42      0.28      0.34      4679\n",
      "           4       0.46      0.25      0.32      8688\n",
      "           5       0.80      0.94      0.87     39602\n",
      "\n",
      "    accuracy                           0.73     61827\n",
      "   macro avg       0.54      0.46      0.48     61827\n",
      "weighted avg       0.69      0.73      0.70     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3744   374   278   115  1133]\n",
      " [  951   568   606   212   877]\n",
      " [  505   375  1332   832  1635]\n",
      " [  208   133   590  2155  5602]\n",
      " [  417   114   332  1375 37364]]\n",
      "Model saved at: /Users/Pratibha/Downloads/Checkpoint_CW2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "train_data['Preprocessed_Review'].fillna('', inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = train_data['Preprocessed_Review']\n",
    "y = train_data['Score']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to train, evaluate, compute confusion matrix, and save model\n",
    "def train_evaluate_confusion_matrix_save_model(vectorizer, classifier, X_train, X_test, y_train, y_test, ngram_range, model_save_path):\n",
    "    # Vectorize the text\n",
    "    vectorizer.set_params(ngram_range=ngram_range)\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    joblib.dump(classifier, model_save_path)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = classifier.predict(X_test_vectorized)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return report, cm\n",
    "\n",
    "# Example usage with different n-gram ranges\n",
    "ngram_ranges = [(1, 1), (1, 2), (1, 3)]  # Unigram, bigram, and trigram\n",
    "model_save_paths = [\"trained_model_LR_CV_unigram.pkl\", \"trained_model_LR_CV_bigram.pkl\", \"trained_model_LR_CV_trigram.pkl\"]\n",
    "\n",
    "for ngram_range, model_save_path in zip(ngram_ranges, model_save_paths):\n",
    "    vectorizer_v9 = CountVectorizer()\n",
    "    classifier_v9 = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    print(f\"Using {vectorizer_v9.__class__.__name__} with {classifier_v9.__class__.__name__} and n-gram range {ngram_range}:\")\n",
    "    #print(f\"Using {vectorizer_v9.__class.name} with {classifier_v9.__class.name_} and n-gram range {ngram_range}:\")\n",
    "    # Train, evaluate, and save the model\n",
    "    report, cm = train_evaluate_confusion_matrix_save_model(vectorizer_v9, classifier_v9, X_train, X_test, y_train, y_test, ngram_range, model_save_path)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Model saved at:\", '/Users/Pratibha/Downloads/Checkpoint_CW2')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yTqUC_BBHbX"
   },
   "source": [
    "## TF-IDF Vectorizer on Logistic regression: (unigram, bigram, trigram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQd4HkqCDFVW",
    "outputId": "ece53d7f-af19-4549-db9e-03947a8699bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TfidfVectorizer with LogisticRegression and n-gram range (1, 1):\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.64      0.63      5644\n",
      "           2       0.35      0.13      0.19      3214\n",
      "           3       0.39      0.24      0.29      4679\n",
      "           4       0.44      0.21      0.28      8688\n",
      "           5       0.78      0.95      0.86     39602\n",
      "\n",
      "    accuracy                           0.72     61827\n",
      "   macro avg       0.52      0.43      0.45     61827\n",
      "weighted avg       0.67      0.72      0.68     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3586   301   267   129  1361]\n",
      " [  890   413   557   208  1146]\n",
      " [  519   282  1102   790  1986]\n",
      " [  216   100   566  1797  6009]\n",
      " [  496    80   308  1129 37589]]\n",
      "\n",
      "Using TfidfVectorizer with LogisticRegression and n-gram range (1, 2):\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.64      0.65      5644\n",
      "           2       0.43      0.12      0.19      3214\n",
      "           3       0.44      0.25      0.32      4679\n",
      "           4       0.47      0.24      0.31      8688\n",
      "           5       0.78      0.96      0.86     39602\n",
      "\n",
      "    accuracy                           0.73     61827\n",
      "   macro avg       0.56      0.44      0.47     61827\n",
      "weighted avg       0.68      0.73      0.69     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3630   250   240   119  1405]\n",
      " [  872   398   584   243  1117]\n",
      " [  460   204  1183   885  1947]\n",
      " [  171    44   466  2050  5957]\n",
      " [  334    39   212  1076 37941]]\n",
      "\n",
      "Using TfidfVectorizer with LogisticRegression and n-gram range (1, 3):\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.63      0.64      5644\n",
      "           2       0.43      0.12      0.18      3214\n",
      "           3       0.43      0.26      0.32      4679\n",
      "           4       0.45      0.27      0.34      8688\n",
      "           5       0.79      0.95      0.86     39602\n",
      "\n",
      "    accuracy                           0.73     61827\n",
      "   macro avg       0.55      0.45      0.47     61827\n",
      "weighted avg       0.68      0.73      0.69     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3579   246   271   163  1385]\n",
      " [  851   372   634   294  1063]\n",
      " [  464   177  1207  1017  1814]\n",
      " [  189    41   473  2323  5662]\n",
      " [  375    38   218  1341 37630]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "train_data['Preprocessed_Review'].fillna('', inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = train_data['Preprocessed_Review']\n",
    "y = train_data['Score']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to train, evaluate, compute confusion matrix, and save model\n",
    "def train_evaluate_confusion_matrix_save_model(vectorizer, classifier, X_train, X_test, y_train, y_test, ngram_range, model_save_path):\n",
    "    # Vectorize the text\n",
    "    vectorizer.set_params(ngram_range=ngram_range)\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    #joblib.dump(classifier, model_save_path)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = classifier.predict(X_test_vectorized)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return report, cm\n",
    "\n",
    "# Example usage with different n-gram ranges and TF-IDF vectorization\n",
    "ngram_ranges = [(1, 1), (1, 2), (1, 3)]  # Unigram, bigram, and trigram\n",
    "model_save_paths = [\"trained_model_LR_TFIDF_unigram.pkl\", \"trained_model_LR_TFIDF_bigram.pkl\", \"trained_model_LR_TFIDF_trigram.pkl\"]\n",
    "\n",
    "for ngram_range, model_save_path in zip(ngram_ranges, model_save_paths):\n",
    "    vectorizer_v10 = TfidfVectorizer()\n",
    "    classifier_v10 = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    print(f\"Using {vectorizer_v10.__class__.__name__} with {classifier_v10.__class__.__name__} and n-gram range {ngram_range}:\")\n",
    "    # Train, evaluate, and save the model\n",
    "    report, cm = train_evaluate_confusion_matrix_save_model(vectorizer_v10, classifier_v10, X_train, X_test, y_train, y_test, ngram_range, model_save_path)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    #print(\"Model saved at:\", model_save_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZs6Wod5xqfS"
   },
   "source": [
    "## HashingVectorizer with Logistic regression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecdu384txsQi",
    "outputId": "fc431725-35f3-4d8e-c070-87fd2a85a431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HashingVectorizer with LogisticRegression:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.63      0.63      5644\n",
      "           2       0.36      0.12      0.18      3214\n",
      "           3       0.40      0.23      0.29      4679\n",
      "           4       0.45      0.20      0.28      8688\n",
      "           5       0.78      0.95      0.86     39602\n",
      "\n",
      "    accuracy                           0.72     61827\n",
      "   macro avg       0.52      0.43      0.45     61827\n",
      "weighted avg       0.67      0.72      0.68     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3581   269   267   118  1409]\n",
      " [  876   387   566   185  1200]\n",
      " [  516   253  1094   762  2054]\n",
      " [  224    79   550  1723  6112]\n",
      " [  519    73   284  1006 37720]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "train_data['Preprocessed_Review'].fillna('', inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = train_data['Preprocessed_Review']\n",
    "y = train_data['Score']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to train, evaluate, and compute confusion matrix\n",
    "def train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test):\n",
    "    # Vectorize the text\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = classifier.predict(X_test_vectorized)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return report, cm\n",
    "\n",
    "# Experiment with different vectorizers and classifiers\n",
    "vectorizers_v2 = [HashingVectorizer()]\n",
    "classifiers_v2 = [LogisticRegression(max_iter=1000)]\n",
    "\n",
    "for vectorizer in vectorizers_v2:\n",
    "    for classifier in classifiers_v2:\n",
    "        print(f\"Using {vectorizer.__class__.__name__} with {classifier.__class__.__name__}:\")\n",
    "        # Train and evaluate with the truncated review data\n",
    "        report, cm = train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test)\n",
    "        print(\"Classification Report:\\n\", report)\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q066APUo1HcY"
   },
   "source": [
    "### FastText Vectorizer with Logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyqFwvI2282K",
    "outputId": "1f5385c4-6ba1-45bd-8904-eb7940d8291a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\rahar\\.conda\\envs\\cuda_gpu_env_tf\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\rahar\\.conda\\envs\\cuda_gpu_env_tf\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\rahar\\.conda\\envs\\cuda_gpu_env_tf\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\rahar\\.conda\\envs\\cuda_gpu_env_tf\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\rahar\\.conda\\envs\\cuda_gpu_env_tf\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pGuWZP43F1S",
    "outputId": "39983a6e-3d33-424c-ce0d-a31a1156e8cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file. This might take a while...\n",
      "Download complete.\n",
      "Unzipping the file...\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 1. Download the file\n",
    "url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n",
    "save_path = \"wiki-news-300d-1M.vec.zip\"\n",
    "\n",
    "print(\"Downloading file. This might take a while...\")\n",
    "urllib.request.urlretrieve(url, save_path)\n",
    "print(\"Download complete.\")\n",
    "\n",
    "# 2. Unzip the file\n",
    "print(\"Unzipping the file...\")\n",
    "with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')  # extracts into current directory\n",
    "print(\"Extraction complete.\")\n",
    "\n",
    "# Optional: remove the .zip file\n",
    "os.remove(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svPGtAV51NMb",
    "outputId": "28d9ebf3-c0c4-4b2f-cd1e-fdcc19b1267e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FastTextVectorizer with LogisticRegression:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.42      0.47      5644\n",
      "           2       0.35      0.03      0.05      3214\n",
      "           3       0.32      0.09      0.14      4679\n",
      "           4       0.37      0.06      0.10      8688\n",
      "           5       0.70      0.97      0.81     39602\n",
      "\n",
      "    accuracy                           0.67     61827\n",
      "   macro avg       0.45      0.31      0.31     61827\n",
      "weighted avg       0.59      0.67      0.59     61827\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 2374    60   147    50  3013]\n",
      " [  645    93   256    72  2148]\n",
      " [  464    60   419   278  3458]\n",
      " [  277    23   273   480  7635]\n",
      " [  719    31   203   417 38232]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class FastTextVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ft_model):\n",
    "        self.ft_model = ft_model\n",
    "        self.vector_size = ft_model.vector_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.transform_text(text) for text in X])\n",
    "\n",
    "    def transform_text(self, text):\n",
    "        words = text.split()\n",
    "        vectors = [self.ft_model[word] for word in words if word in self.ft_model]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.vector_size)\n",
    "\n",
    "\n",
    "# Load the FastText model with a limit\n",
    "ft_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\")\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "train_data['Preprocessed_Review'].fillna('', inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = train_data['Preprocessed_Review']\n",
    "y = train_data['Score']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to train, evaluate, and compute confusion matrix\n",
    "def train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test):\n",
    "    # Vectorize the text\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = classifier.predict(X_test_vectorized)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return report, cm\n",
    "\n",
    "# Experiment with FastTextVectorizer and Logistic Regression classifier\n",
    "vectorizer = FastTextVectorizer(ft_model)\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "print(f\"Using FastTextVectorizer with {classifier.__class__.__name__}:\")\n",
    "# Train and evaluate with the truncated review data\n",
    "report, cm = train_evaluate_confusion_matrix(vectorizer, classifier, X_train, X_test, y_train, y_test)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "hhWi58gE_yLm",
    "outputId": "97adeb2f-6d24-43eb-8d3f-8d96701c7004"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cc7d3 td:hover {\n",
       "  background-color: #ffffb3;\n",
       "}\n",
       "#T_cc7d3 .index_name {\n",
       "  font-style: italic;\n",
       "  color: darkgrey;\n",
       "  font-weight: normal;\n",
       "}\n",
       "#T_cc7d3 th:not(.index_name) {\n",
       "  background-color: #000066;\n",
       "  color: white;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_cc7d3 .col_heading {\n",
       "  font-size: 22px;\n",
       "  text-align: center;\n",
       "  border: none;\n",
       "}\n",
       "#T_cc7d3 .row_heading {\n",
       "  font-size: 22px;\n",
       "  text-align: center;\n",
       "  border: none;\n",
       "}\n",
       "#T_cc7d3 .data {\n",
       "  font-size: 22px;\n",
       "  text-align: center;\n",
       "  border: none;\n",
       "}\n",
       "#T_cc7d3 td.row0.col0 {\n",
       "  background-color: green;\n",
       "  color: white;\n",
       "}\n",
       "#T_cc7d3 caption {\n",
       "  caption-side: top;\n",
       "  font-size: 20px;\n",
       "  color: black;\n",
       "  font-weight: bold;\n",
       "  text-align: center;\n",
       "  margin-bottom: 30px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cc7d3\">\n",
       "  <caption>Comparison of HashingVectorizer and FastText Vectorizer using Logistic Regression</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cc7d3_level0_col0\" class=\"col_heading level0 col0\" >('HashingVectorizer',)</th>\n",
       "      <th id=\"T_cc7d3_level0_col1\" class=\"col_heading level0 col1\" >('FastText Vectorizer',)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cc7d3_level0_row0\" class=\"row_heading level0 row0\" >Logistic Regression</th>\n",
       "      <td id=\"T_cc7d3_row0_col0\" class=\"data row0 col0\" >72%</td>\n",
       "      <td id=\"T_cc7d3_row0_col1\" class=\"data row0 col1\" >67%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x288426ad690>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe for accuracy scores of two vectorizers\n",
    "df_accuracy = pd.DataFrame([\n",
    "    ['72%', '67%'],\n",
    "], index=pd.Index(['Logistic Regression']),\n",
    "   columns=pd.MultiIndex.from_product([['HashingVectorizer', 'FastText Vectorizer']]))\n",
    "df_accuracy.style\n",
    "\n",
    "\n",
    "s_accuracy = df_accuracy.style\n",
    "#s_f1 = df_f1.style\n",
    "# Add table styles\n",
    "cell_hover = {'selector': 'td:hover', 'props': [('background-color', '#ffffb3')]}\n",
    "index_names = {'selector': '.index_name', 'props': 'font-style: italic; color: darkgrey; font-weight: normal;'}\n",
    "headers = {'selector': 'th:not(.index_name)', 'props': 'background-color: #000066; color: white; text-align: center'}\n",
    "s_accuracy.set_table_styles([cell_hover, index_names, headers])\n",
    "s_accuracy.set_table_styles([\n",
    "        {'selector': '.col_heading', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
    "    {'selector': '.row_heading', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
    "    {'selector': '.data', 'props': 'font-size: 22px; text-align: center; border: none;'},\n",
    "    {'selector': 'td.row0.col0',  'props': 'background-color: green; color: white'},\n",
    "#     {'selector': 'td.row1.col1',  'props': 'background-color: green; color: white'},\n",
    "], overwrite=False)\n",
    "s_accuracy.set_caption(\"Comparison of HashingVectorizer and FastText Vectorizer using Logistic Regression\") \\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'caption',\n",
    "        'props': 'caption-side: top; font-size: 20px; color: black; font-weight: bold; text-align: center; margin-bottom: 30px'\n",
    "    }], overwrite=False)\n",
    "\n",
    "# s_accuracy.format('{:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZXPR8diu8PW"
   },
   "source": [
    "## Vector Space Model for sequential models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmTbn_8Cu-Zm"
   },
   "source": [
    "## Word2Vec model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwRRyHimvD0X"
   },
   "source": [
    "### Creates Word2Vec using only minimal processing - CBOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZTGLKPDvIln",
    "outputId": "30d9b4ad-c373-4b54-cde5-92c85d6b8719"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\AppData\\Local\\Temp\\ipykernel_12528\\3431769336.py:31: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(raw_text, 'html.parser').get_text()  #remove html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show a cleaned review in the training set : \n",
      " we were house sitting for our neighbors the past couple of weeks and found this item that we thought might come in handy especially with their pups or dog and pup don t know the age limit for canines anyway chef michael s grilled sirloin literally drove them wild their owners feed them purina dog chow but once we opened the bag and pour a bowl full for each they dove right in and went scurrying to the bag when we put it down something that didn t happen with purina when they came back our neighbor ricardo commented on how much the dogs seems to prefer the chef michael s now i don t know what they re paying for the purina or if the chef michael s would be a treat here and there but one thing is certain the chef michael s grilled sirloin made quite an impression on those dogs they really really love the taste of this stuff\n",
      "247304 parsed sentence in the training set\n",
      "\n",
      "Show a parsed sentence in the training set : \n",
      " ['we', 'were', 'house', 'sitting', 'for', 'our', 'neighbors', 'the', 'past', 'couple', 'of', 'weeks', 'and', 'found', 'this', 'item', 'that', 'we', 'thought', 'might', 'come', 'in', 'handy', 'especially', 'with', 'their', 'pups', 'or', 'dog', 'and', 'pup', 'don', 't', 'know', 'the', 'age', 'limit', 'for', 'canines', 'anyway', 'chef', 'michael', 's', 'grilled', 'sirloin', 'literally', 'drove', 'them', 'wild', 'their', 'owners', 'feed', 'them', 'purina', 'dog', 'chow', 'but', 'once', 'we', 'opened', 'the', 'bag', 'and', 'pour', 'a', 'bowl', 'full', 'for', 'each', 'they', 'dove', 'right', 'in', 'and', 'went', 'scurrying', 'to', 'the', 'bag', 'when', 'we', 'put', 'it', 'down', 'something', 'that', 'didn', 't', 'happen', 'with', 'purina', 'when', 'they', 'came', 'back', 'our', 'neighbor', 'ricardo', 'commented', 'on', 'how', 'much', 'the', 'dogs', 'seems', 'to', 'prefer', 'the', 'chef', 'michael', 's', 'now', 'i', 'don', 't', 'know', 'what', 'they', 're', 'paying', 'for', 'the', 'purina', 'or', 'if', 'the', 'chef', 'michael', 's', 'would', 'be', 'a', 'treat', 'here', 'and', 'there', 'but', 'one', 'thing', 'is', 'certain', 'the', 'chef', 'michael', 's', 'grilled', 'sirloin', 'made', 'quite', 'an', 'impression', 'on', 'those', 'dogs', 'they', 'really', 'really', 'love', 'the', 'taste', 'of', 'this', 'stuff']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load preprocessed data\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Extracting features (X) and labels (y)\n",
    "X = data['Text'].astype(str)\n",
    "y = data['Score']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'html.parser').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case\n",
    "\n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    if stemming==True: # stemming\n",
    "#         stemmer = PorterStemmer()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    if split_text==True:  # split text\n",
    "        return (words)\n",
    "\n",
    "    return( \" \".join(words))\n",
    "\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
    "\n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    Parse text into sentences\n",
    "    '''\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Parse each review in the training set into sentences\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer)\n",
    "\n",
    "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
    "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-3Z0zDR03Pz",
    "outputId": "071284fc-177d-41ed-e5cf-c46a880e3654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model ...\n",
      "\n",
      "Number of words in the vocabulary list : 20599 \n",
      "\n",
      "Show first 10 words in the vocalbulary list  vocabulary list: \n",
      " ['the', 'i', 'and', 'a', 'it', 'to', 'of', 'is', 'this', 'in']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\AppData\\Local\\Temp\\ipykernel_12528\\2205193108.py:13: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "# Fit parsed sentences to Word2Vec model\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "num_features = 300  #embedding dimension\n",
    "min_word_count = 10\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "w2v = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count,\n",
    "                 window = context, sample = downsampling)\n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\n",
    "\n",
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index_to_key)) #4016\n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index_to_key[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqRFllHk3bLq"
   },
   "source": [
    "### Creates Word2Vec model using minimal preprocessing - SkipGram:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b76pM6zo3kSr",
    "outputId": "91f06f6b-1a3e-40d3-c4a2-9f82990cac73"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SnowballStemmer, WordNetLemmatizer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize, word_tokenize, pos_tag\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     12\u001b[0m en_nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import spacy\n",
    "\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load preprocessed data\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Extracting features (X) and labels (y)\n",
    "X = data['Text'].astype(str)\n",
    "y = data['Score']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, lemmatization = True, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'html.parser').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case\n",
    "\n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    if stemming==True: # stemming\n",
    "#         stemmer = PorterStemmer()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    if lemmatization:  # Lemmatization\n",
    "        lemmatized_tokens = []\n",
    "        for token in en_nlp(\" \".join(words)):\n",
    "            if token.is_alpha and not token.is_stop:\n",
    "                pos_tag = token.pos_\n",
    "                wn_pos_tag = get_wordnet_pos(pos_tag)\n",
    "                if wn_pos_tag is not None:\n",
    "                    lemmatized_tokens.append(token.lemma_)\n",
    "                else:\n",
    "                    lemmatized_tokens.append(token.text)\n",
    "        words = lemmatized_tokens\n",
    "\n",
    "    if split_text==True:  # split text\n",
    "        return (words)\n",
    "\n",
    "    return( \" \".join(words))\n",
    "\n",
    "def get_wordnet_pos(pos):\n",
    "        if pos.startswith('ADJ'):\n",
    "            return 'a'  # Adjective\n",
    "        elif pos.startswith('ADV'):\n",
    "            return 'r'  # Adverb\n",
    "        elif pos.startswith('NOUN'):\n",
    "            return 'n'  # Noun\n",
    "        elif pos.startswith('VERB'):\n",
    "            return 'v'  # Verb\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "\n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    Parse text into sentences\n",
    "    '''\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Parse each review in the training set into sentences\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrjVpMqs4vOl"
   },
   "source": [
    "### Creates Word2Vec using minimal and Stemming processing - CBOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e57943b",
    "outputId": "26675320-caed-477f-eaab-4c518d4b6f3b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import spacy\n",
    "\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load preprocessed data\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Extracting features (X) and labels (y)\n",
    "X = data['Text'].astype(str)\n",
    "y = data['Score']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load spaCy English language model\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, lemmatization = False, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case\n",
    "\n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    if stemming==True: # stemming\n",
    "#         stemmer = PorterStemmer()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    if lemmatization:  # Lemmatization\n",
    "        lemmatized_tokens = []\n",
    "        for token in en_nlp(\" \".join(words)):\n",
    "            if token.is_alpha and not token.is_stop:\n",
    "                pos_tag = token.pos_\n",
    "                wn_pos_tag = get_wordnet_pos(pos_tag)\n",
    "                if wn_pos_tag is not None:\n",
    "                    lemmatized_tokens.append(token.lemma_)\n",
    "                else:\n",
    "                    lemmatized_tokens.append(token.text)\n",
    "        words = lemmatized_tokens\n",
    "\n",
    "    if split_text==True:  # split text\n",
    "        return (words)\n",
    "\n",
    "    return( \" \".join(words))\n",
    "\n",
    "def get_wordnet_pos(pos):\n",
    "        if pos.startswith('ADJ'):\n",
    "            return 'a'  # Adjective\n",
    "        elif pos.startswith('ADV'):\n",
    "            return 'r'  # Adverb\n",
    "        elif pos.startswith('NOUN'):\n",
    "            return 'n'  # Noun\n",
    "        elif pos.startswith('VERB'):\n",
    "            return 'v'  # Verb\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    Parse text into sentences\n",
    "    '''\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords=False, stemming=True, lemmatization = False, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
    "\n",
    "# for d in X_test:\n",
    "#     X_test_cleaned.append(cleanText(d))\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Parse each review in the training set into sentences\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer)\n",
    "\n",
    "# sentences = []\n",
    "# for review in X_test_cleaned:\n",
    "#     sentences += parseSent(review, tokenizer)\n",
    "num_features = 300  #embedding dimension\n",
    "min_word_count = 10\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "w2v = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count,\n",
    "                 window = context, sample = downsampling)\n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"w2v_model\") #save trained word2vec model\n",
    "\n",
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index_to_key)) #4016\n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index_to_key[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT5oCHTMdxA8"
   },
   "source": [
    "### Creates Word2Vec model using minimal and stemming preprocessing - SkipGram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "sBSZ1y7XeJNW",
    "outputId": "a36c7ab5-1117-4980-9a42-d55c31c0cbb8"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import spacy\n",
    "\n",
    "# Load preprocessed data\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Extracting features (X) and labels (y)\n",
    "X = data['Text'].astype(str)\n",
    "y = data['Score']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load spaCy English language model\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, lemmatization = False, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case\n",
    "\n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    if stemming==True: # stemming\n",
    "#         stemmer = PorterStemmer()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    if lemmatization:  # Lemmatization\n",
    "        lemmatized_tokens = []\n",
    "        for token in en_nlp(\" \".join(words)):\n",
    "            if token.is_alpha and not token.is_stop:\n",
    "                pos_tag = token.pos_\n",
    "                wn_pos_tag = get_wordnet_pos(pos_tag)\n",
    "                if wn_pos_tag is not None:\n",
    "                    lemmatized_tokens.append(token.lemma_)\n",
    "                else:\n",
    "                    lemmatized_tokens.append(token.text)\n",
    "        words = lemmatized_tokens\n",
    "\n",
    "    if split_text==True:  # split text\n",
    "        return (words)\n",
    "\n",
    "    return( \" \".join(words))\n",
    "\n",
    "def get_wordnet_pos(pos):\n",
    "        if pos.startswith('ADJ'):\n",
    "            return 'a'  # Adjective\n",
    "        elif pos.startswith('ADV'):\n",
    "            return 'r'  # Adverb\n",
    "        elif pos.startswith('NOUN'):\n",
    "            return 'n'  # Noun\n",
    "        elif pos.startswith('VERB'):\n",
    "            return 'v'  # Verb\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    Parse text into sentences\n",
    "    '''\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords=False, stemming=True, lemmatization = False, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
    "\n",
    "# for d in X_test:\n",
    "#     X_test_cleaned.append(cleanText(d))\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Parse each review in the training set into sentences\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer)\n",
    "\n",
    "# sentences = []\n",
    "# for review in X_test_cleaned:\n",
    "#     sentences += parseSent(review, tokenizer)\n",
    "num_features = 300  #embedding dimension\n",
    "min_word_count = 10\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "w2v = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count,\n",
    "                 window = context, sample = downsampling, sg=1)\n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"word2vec_models_SG\") #save trained word2vec model\n",
    "\n",
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index_to_key)) #4016\n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index_to_key[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8fevH4tgVjw"
   },
   "source": [
    "### Creates Word2Vec using minimal and Lemmatizer processing - CBOW:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "NpVxFvNigpge",
    "outputId": "145a845d-ded8-4f7f-e992-ac06808dfed1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import spacy\n",
    "\n",
    "# Load spaCy English language model\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load preprocessed data\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Extracting features (X) and labels (y)\n",
    "X = data['Text'].astype(str)\n",
    "y = data['Score']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def cleanText(raw_text, remove_stopwords=False, lemmatization=False, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case\n",
    "\n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "#     if stemming==True: # stemming\n",
    "# #         stemmer = PorterStemmer()\n",
    "#         stemmer = SnowballStemmer('english')\n",
    "#         words = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    if lemmatization:  # Lemmatization\n",
    "        lemmatized_tokens = []\n",
    "        for token in en_nlp(\" \".join(words)):\n",
    "            if token.is_alpha and not token.is_stop:\n",
    "                pos_tag = token.pos_\n",
    "                wn_pos_tag = get_wordnet_pos(pos_tag)\n",
    "                if wn_pos_tag is not None:\n",
    "                    lemmatized_tokens.append(token.lemma_)\n",
    "                else:\n",
    "                    lemmatized_tokens.append(token.text)\n",
    "        words = lemmatized_tokens\n",
    "    if split_text==True:  # split text\n",
    "        return (words)\n",
    "\n",
    "    return( \" \".join(words))\n",
    "\n",
    "def get_wordnet_pos(pos):\n",
    "        if pos.startswith('ADJ'):\n",
    "            return 'a'  # Adjective\n",
    "        elif pos.startswith('ADV'):\n",
    "            return 'r'  # Adverb\n",
    "        elif pos.startswith('NOUN'):\n",
    "            return 'n'  # Noun\n",
    "        elif pos.startswith('VERB'):\n",
    "            return 'v'  # Verb\n",
    "        else:\n",
    "            return None\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
    "\n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    Parse text into sentences\n",
    "    '''\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, lemmatization=True, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Parse each review in the training set into sentences\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer)\n",
    "\n",
    "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
    "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGxsgTcsAIwx"
   },
   "source": [
    "# Part - 4: Model training, selection and hyperparameter tuning and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rErw7BvuALZD",
    "outputId": "f051e3cc-04bd-46d4-f221-86812ce57bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: MultinomialNB\n",
      "vectorizer CountVectorizer_Unigram_max_df_0.2\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.663 total time=   4.4s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.663 total time=   4.2s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.680 total time=   4.2s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=   4.3s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.683 total time=   4.2s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.684 total time=   4.2s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer CountVectorizer_Bigram_max_df_0.2\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.690 total time=  13.9s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.690 total time=  13.9s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.680 total time=  13.9s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=  14.0s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.653 total time=  14.1s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.653 total time=  14.0s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer CountVectorizer_Trigram_max_df_0.2\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.692 total time=  29.7s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.691 total time=  29.6s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.656 total time=  29.8s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.656 total time=  29.5s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.643 total time=  29.5s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.643 total time=  29.5s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer TfidfVectorizer_Unigram_max_df_0.2\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.658 total time=   4.5s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.657 total time=   4.5s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   4.5s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   4.5s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   4.6s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   4.5s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer TfidfVectorizer_Bigram_max_df_0.2\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.648 total time=  14.8s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.647 total time=  14.8s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  15.2s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  14.8s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  14.8s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  15.1s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer TfidfVectorizer_Trigram_max_df_0.2\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  30.8s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  30.9s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  31.0s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  31.1s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  31.0s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  30.9s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer CountVectorizer_Unigram_max_df_0.5\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.667 total time=   4.4s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.665 total time=   4.5s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.683 total time=   4.4s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.682 total time=   4.4s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.686 total time=   4.5s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.686 total time=   4.4s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer CountVectorizer_Bigram_max_df_0.5\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.692 total time=  14.3s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.691 total time=  14.4s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=  14.4s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=  14.5s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.652 total time=  14.6s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.651 total time=  14.4s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer CountVectorizer_Trigram_max_df_0.5\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.693 total time=  29.3s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.692 total time=  29.8s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.655 total time=  29.9s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.654 total time=  29.7s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.642 total time=  29.7s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.643 total time=  30.0s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer TfidfVectorizer_Unigram_max_df_0.5\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.658 total time=   4.6s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.658 total time=   4.6s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   4.6s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   4.5s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   4.7s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   4.5s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer TfidfVectorizer_Bigram_max_df_0.5\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.648 total time=  15.2s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.647 total time=  15.4s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  16.1s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  16.2s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  16.1s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  15.9s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer TfidfVectorizer_Trigram_max_df_0.5\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  32.3s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  32.9s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  32.4s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  32.7s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  32.8s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  32.9s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer CountVectorizer_Unigram_max_df_1.0\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.667 total time=   4.6s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.665 total time=   4.7s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.683 total time=   4.8s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.682 total time=   4.7s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.686 total time=   4.5s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.686 total time=   4.6s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer CountVectorizer_Bigram_max_df_1.0\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.692 total time=  15.0s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.691 total time=  15.1s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=  15.1s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.679 total time=  15.0s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.652 total time=  15.0s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.651 total time=  14.9s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer CountVectorizer_Trigram_max_df_1.0\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.693 total time=  31.2s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.692 total time=  31.7s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.655 total time=  29.9s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.654 total time=  29.7s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.642 total time=  29.9s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.643 total time=  29.4s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer TfidfVectorizer_Unigram_max_df_1.0\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.658 total time=   4.6s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.658 total time=   4.5s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   4.4s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.640 total time=   4.4s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   4.5s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=   4.5s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer TfidfVectorizer_Bigram_max_df_1.0\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.648 total time=  15.0s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.647 total time=  15.2s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  14.9s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  15.0s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  15.3s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  15.1s\n",
      "Classifier: MultinomialNB\n",
      "vectorizer TfidfVectorizer_Trigram_max_df_1.0\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  31.0s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.1;, score=0.642 total time=  31.2s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  31.2s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=0.5;, score=0.637 total time=  30.6s\n",
      "[CV 1/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  30.8s\n",
      "[CV 2/2] END classifier=MultinomialNB(), classifier__alpha=1.0;, score=0.637 total time=  30.9s\n",
      "Classifier: LogisticRegression\n",
      "vectorizer CountVectorizer_Unigram_max_df_0.2\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=   2.2s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=   2.3s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.698 total time=  12.2s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.699 total time=  12.3s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=   2.2s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=   2.2s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.703 total time=  25.5s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.703 total time=  23.9s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=   2.2s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=   2.2s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.692 total time=  54.1s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.692 total time=  54.1s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=   2.2s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=   2.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.665 total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.665 total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "8 fits failed out of a total of 16.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.69818523        nan 0.70290816        nan 0.69184485\n",
      "        nan 0.66456669]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LogisticRegression\n",
      "vectorizer CountVectorizer_Bigram_max_df_0.2\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=   8.4s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=   8.3s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.709 total time= 1.8min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.709 total time= 1.8min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=   8.3s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=   8.5s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.719 total time= 3.5min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.719 total time= 3.6min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=   8.4s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=   8.4s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.713 total time= 5.7min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.714 total time= 5.6min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=   8.4s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=   8.3s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.707 total time= 7.2min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.709 total time= 7.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "8 fits failed out of a total of 16.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.70882396        nan 0.71896532        nan 0.7131304\n",
      "        nan 0.70812846]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LogisticRegression\n",
      "vectorizer CountVectorizer_Trigram_max_df_0.2\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=  21.4s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=  21.7s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.709 total time= 4.6min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.709 total time= 5.2min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=  20.3s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=  20.3s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.720 total time= 9.1min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.721 total time= 8.8min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=  20.4s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=  20.3s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.719 total time=12.8min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.720 total time=13.0min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=  21.2s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=  21.4s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.717 total time=18.5min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.720 total time=19.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "8 fits failed out of a total of 16.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.70875117        nan 0.7203078         nan 0.71983874\n",
      "        nan 0.71849626]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LogisticRegression\n",
      "vectorizer TfidfVectorizer_Unigram_max_df_0.2\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=   2.4s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=   2.4s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.638 total time=   8.2s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.638 total time=   7.9s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=   2.3s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=   2.3s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.689 total time=  14.1s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.688 total time=  13.9s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=   2.4s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=   2.4s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.708 total time=  30.4s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.709 total time=  30.5s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=   2.4s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=   2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.694 total time= 1.2min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.695 total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "8 fits failed out of a total of 16.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.63823068        nan 0.6887515         nan 0.70867434\n",
      "        nan 0.69414971]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LogisticRegression\n",
      "vectorizer TfidfVectorizer_Bigram_max_df_0.2\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=   9.1s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l1;, score=nan total time=   9.0s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.637 total time=  48.2s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.637 total time=  50.6s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=   9.1s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l1;, score=nan total time=   8.9s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.668 total time= 1.8min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.667 total time= 1.9min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=   8.9s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l1;, score=nan total time=   9.2s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.718 total time= 4.6min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.719 total time= 4.8min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=   8.9s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l1;, score=nan total time=   9.1s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.720 total time=10.5min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import os\n",
    "\n",
    "# Create directory for saving models\n",
    "os.makedirs(\"./grid_search_models\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "train_data['Preprocessed_Review'] = train_data['Preprocessed_Review'].fillna('')\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = train_data['Preprocessed_Review']\n",
    "y = train_data['Score']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a list of classifiers with their associated parameters\n",
    "classifiers = [\n",
    "    {\n",
    "        'classifier': [MultinomialNB()],\n",
    "        'classifier__alpha': [0.1, 0.5, 1.0],  # Smoothing parameter for MultinomialNB\n",
    "    },\n",
    "]\n",
    "max_df_values = [0.2, 0.5, 1.0]\n",
    "vectorizers = []\n",
    "for max_df in max_df_values:\n",
    "    vectorizers.extend([\n",
    "        (CountVectorizer(ngram_range=(1, 1), max_df=max_df), f\"CountVectorizer_Unigram_max_df_{max_df}\"),\n",
    "        (CountVectorizer(ngram_range=(1, 2), max_df=max_df), f\"CountVectorizer_Bigram_max_df_{max_df}\"),\n",
    "        (CountVectorizer(ngram_range=(1, 3), max_df=max_df), f\"CountVectorizer_Trigram_max_df_{max_df}\"),\n",
    "        (TfidfVectorizer(ngram_range=(1, 1), max_df=max_df), f\"TfidfVectorizer_Unigram_max_df_{max_df}\"),\n",
    "        (TfidfVectorizer(ngram_range=(1, 2), max_df=max_df), f\"TfidfVectorizer_Bigram_max_df_{max_df}\"),\n",
    "        (TfidfVectorizer(ngram_range=(1, 3), max_df=max_df), f\"TfidfVectorizer_Trigram_max_df_{max_df}\"),\n",
    "    ])\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform grid search for each combination of classifier and vectorizer\n",
    "for classifier_config in classifiers:\n",
    "    for vectorizer, vectorizer_name in vectorizers:\n",
    "        print(\"Classifier:\", type(classifier_config['classifier'][0]).__name__)\n",
    "        print(\"vectorizer\", vectorizer_name)\n",
    "        # Create a pipeline with the current vectorizer and classifier\n",
    "        pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier_config['classifier']),\n",
    "        ])\n",
    "\n",
    "        # Create GridSearchCV object\n",
    "        grid_search = GridSearchCV(pipeline, classifier_config, cv=2, scoring='accuracy', verbose=3)\n",
    "\n",
    "        # Perform grid search\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Save the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        model_filename = f\"./grid_search_models/{vectorizer_name}_{type(classifier_config['classifier'][0]).__name__}_best_model.joblib\"\n",
    "        dump(best_model, model_filename)\n",
    "\n",
    "        # Get best parameters and best score\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "        # Store the results\n",
    "        result = {\n",
    "            'vectorizer': vectorizer_name,\n",
    "            'classifier': type(classifier_config['classifier'][0]).__name__,\n",
    "            'best_params': best_params,\n",
    "            'best_score': best_score,\n",
    "            'test_score': test_score,\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(\"===================================================\")\n",
    "    print(f\"Vectorizer: {result['vectorizer']}, Classifier: {result['classifier']}\")\n",
    "    print(\"Best parameters:\", result['best_params'])\n",
    "    print(\"Best cross-validation score:\", result['best_score'])\n",
    "    print(\"Test set accuracy:\", result['test_score'])\n",
    "    print()\n",
    "\n",
    "    # Print classification report and confusion matrix\n",
    "    print(\"Classification Report:\")\n",
    "    y_pred = result['best_model'].predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"===================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LogisticRegression\n",
      "vectorizer CountVectorizer_Unigram_max_df_0.2\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.698 total time=  13.3s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.699 total time=  13.3s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.703 total time=  28.0s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.703 total time=  25.9s\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.692 total time=  59.5s\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.692 total time=  59.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.665 total time= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahar\\.conda\\envs\\tf210_gpu_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.665 total time= 1.3min\n",
      "Classifier: LogisticRegression\n",
      "vectorizer CountVectorizer_Bigram_max_df_0.2\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.709 total time= 2.1min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.01, classifier__penalty=l2;, score=0.709 total time= 2.1min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.719 total time= 4.1min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=0.1, classifier__penalty=l2;, score=0.719 total time= 4.2min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.713 total time= 6.3min\n",
      "[CV 2/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=1.0, classifier__penalty=l2;, score=0.714 total time= 6.3min\n",
      "[CV 1/2] END classifier=LogisticRegression(max_iter=1000), classifier__C=10.0, classifier__penalty=l2;, score=0.707 total time= 8.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import os\n",
    "\n",
    "# Create directory for saving models\n",
    "os.makedirs(\"./grid_search_models\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "train_data['Preprocessed_Review'] = train_data['Preprocessed_Review'].fillna('')\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = train_data['Preprocessed_Review']\n",
    "y = train_data['Score']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a list of classifiers with their associated parameters\n",
    "classifiers = [\n",
    "    {\n",
    "        'classifier': [LogisticRegression(max_iter=1000)],\n",
    "        'classifier__C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'classifier__penalty': ['l2'], # Only 'l2' here\n",
    "    },\n",
    "]\n",
    "max_df_values = [0.2, 0.5, 1.0]\n",
    "vectorizers = []\n",
    "for max_df in max_df_values:\n",
    "    vectorizers.extend([\n",
    "        (CountVectorizer(ngram_range=(1, 1), max_df=max_df), f\"CountVectorizer_Unigram_max_df_{max_df}\"),\n",
    "        (CountVectorizer(ngram_range=(1, 2), max_df=max_df), f\"CountVectorizer_Bigram_max_df_{max_df}\"),\n",
    "        (CountVectorizer(ngram_range=(1, 3), max_df=max_df), f\"CountVectorizer_Trigram_max_df_{max_df}\"),\n",
    "        (TfidfVectorizer(ngram_range=(1, 1), max_df=max_df), f\"TfidfVectorizer_Unigram_max_df_{max_df}\"),\n",
    "        (TfidfVectorizer(ngram_range=(1, 2), max_df=max_df), f\"TfidfVectorizer_Bigram_max_df_{max_df}\"),\n",
    "        (TfidfVectorizer(ngram_range=(1, 3), max_df=max_df), f\"TfidfVectorizer_Trigram_max_df_{max_df}\"),\n",
    "    ])\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform grid search for each combination of classifier and vectorizer\n",
    "for classifier_config in classifiers:\n",
    "    for vectorizer, vectorizer_name in vectorizers:\n",
    "        print(\"Classifier:\", type(classifier_config['classifier'][0]).__name__)\n",
    "        print(\"vectorizer\", vectorizer_name)\n",
    "        # Create a pipeline with the current vectorizer and classifier\n",
    "        pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier_config['classifier']),\n",
    "        ])\n",
    "\n",
    "        # Create GridSearchCV object\n",
    "        grid_search = GridSearchCV(pipeline, classifier_config, cv=2, scoring='accuracy', verbose=3)\n",
    "\n",
    "        # Perform grid search\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Save the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        model_filename = f\"./grid_search_models/{vectorizer_name}_{type(classifier_config['classifier'][0]).__name__}_best_model.joblib\"\n",
    "        dump(best_model, model_filename)\n",
    "\n",
    "        # Get best parameters and best score\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "        # Store the results\n",
    "        result = {\n",
    "            'vectorizer': vectorizer_name,\n",
    "            'classifier': type(classifier_config['classifier'][0]).__name__,\n",
    "            'best_params': best_params,\n",
    "            'best_score': best_score,\n",
    "            'test_score': test_score,\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(\"===================================================\")\n",
    "    print(f\"Vectorizer: {result['vectorizer']}, Classifier: {result['classifier']}\")\n",
    "    print(\"Best parameters:\", result['best_params'])\n",
    "    print(\"Best cross-validation score:\", result['best_score'])\n",
    "    print(\"Test set accuracy:\", result['test_score'])\n",
    "    print()\n",
    "\n",
    "    # Print classification report and confusion matrix\n",
    "    print(\"Classification Report:\")\n",
    "    y_pred = result['best_model'].predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"===================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cuml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# ----------------- GPU-based Classifiers from cuML -----------------\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make sure you have RAPIDS installed (e.g., `conda install rapids` in a suitable environment)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB \u001b[38;5;28;01mas\u001b[39;00m CumlMultinomialNB\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcuml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression \u001b[38;5;28;01mas\u001b[39;00m CumlLogisticRegression\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#from cuml.ensemble import GradientBoostingClassifier as CumlGradientBoostingClassifier\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create directory for saving models\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cuml'"
     ]
    }
   ],
   "source": [
    "# ====================== GPU Version Example ======================\n",
    "# Replace the CPU-based scikit-learn estimators with their GPU-based cuML equivalents.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from joblib import dump\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ----------------- GPU-based Classifiers from cuML -----------------\n",
    "# Make sure you have RAPIDS installed (e.g., `conda install rapids` in a suitable environment)\n",
    "from cuml.naive_bayes import MultinomialNB as CumlMultinomialNB\n",
    "from cuml.linear_model import LogisticRegression as CumlLogisticRegression\n",
    "#from cuml.ensemble import GradientBoostingClassifier as CumlGradientBoostingClassifier\n",
    "\n",
    "# Create directory for saving models\n",
    "os.makedirs(\"./grid_search_models\", exist_ok=True)\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "train_data['Preprocessed_Review'] = train_data['Preprocessed_Review'].fillna('')\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = train_data['Preprocessed_Review']\n",
    "y = train_data['Score']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a list of GPU-based classifiers with parameter grids\n",
    "classifiers = [\n",
    "    {\n",
    "        'classifier': [CumlMultinomialNB()],\n",
    "        # Smoothing parameter for MultinomialNB; note that cuML's MultinomialNB also accepts 'alpha'\n",
    "        'classifier__alpha': [0.1, 0.5, 1.0],\n",
    "    },\n",
    "    {\n",
    "        'classifier': [CumlLogisticRegression(max_iter=1000)],\n",
    "        # Regularization parameter and penalty for cuML LogisticRegression\n",
    "        # (verify that your cuML version supports 'l1' and the C ranges you specify)\n",
    "        'classifier__C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create list of (vectorizer, name) pairs\n",
    "max_df_values = [0.2, 0.5, 1.0]\n",
    "vectorizers = []\n",
    "for max_df in max_df_values:\n",
    "    vectorizers.extend([\n",
    "        (CountVectorizer(ngram_range=(1, 1), max_df=max_df),\n",
    "         f\"CountVectorizer_Unigram_max_df_{max_df}\"),\n",
    "        (CountVectorizer(ngram_range=(1, 2), max_df=max_df),\n",
    "         f\"CountVectorizer_Bigram_max_df_{max_df}\"),\n",
    "        (CountVectorizer(ngram_range=(1, 3), max_df=max_df),\n",
    "         f\"CountVectorizer_Trigram_max_df_{max_df}\"),\n",
    "        (TfidfVectorizer(ngram_range=(1, 1), max_df=max_df),\n",
    "         f\"TfidfVectorizer_Unigram_max_df_{max_df}\"),\n",
    "        (TfidfVectorizer(ngram_range=(1, 2), max_df=max_df),\n",
    "         f\"TfidfVectorizer_Bigram_max_df_{max_df}\"),\n",
    "        (TfidfVectorizer(ngram_range=(1, 3), max_df=max_df),\n",
    "         f\"TfidfVectorizer_Trigram_max_df_{max_df}\"),\n",
    "    ])\n",
    "\n",
    "results = []\n",
    "\n",
    "# Perform grid search for each combination of classifier and vectorizer\n",
    "for classifier_config in classifiers:\n",
    "    for vectorizer, vectorizer_name in vectorizers:\n",
    "        print(\"Classifier:\", type(classifier_config['classifier'][0]).__name__)\n",
    "        print(\"Vectorizer:\", vectorizer_name)\n",
    "\n",
    "        # Create a pipeline with the current vectorizer and classifier\n",
    "        pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier_config['classifier'][0]),\n",
    "        ])\n",
    "\n",
    "        # Create GridSearchCV object\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            classifier_config,\n",
    "            cv=2,\n",
    "            scoring='accuracy',\n",
    "            verbose=3\n",
    "        )\n",
    "\n",
    "        # Fit grid search\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Save the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        model_filename = (\n",
    "            f\"./grid_search_models/\"\n",
    "            f\"{vectorizer_name}_\"\n",
    "            f\"{type(classifier_config['classifier'][0]).__name__}_best_model.joblib\"\n",
    "        )\n",
    "        dump(best_model, model_filename)\n",
    "\n",
    "        # Collect best params, cross-val score, and test score\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "        test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "        # Store the results\n",
    "        result = {\n",
    "            'vectorizer': vectorizer_name,\n",
    "            'classifier': type(classifier_config['classifier'][0]).__name__,\n",
    "            'best_params': best_params,\n",
    "            'best_score': best_score,\n",
    "            'test_score': test_score,\n",
    "            # Save the best model object so we can report classification metrics\n",
    "            'best_model': best_model\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(\"===================================================\")\n",
    "    print(f\"Vectorizer: {result['vectorizer']}, Classifier: {result['classifier']}\")\n",
    "    print(\"Best parameters:\", result['best_params'])\n",
    "    print(\"Best cross-validation score:\", result['best_score'])\n",
    "    print(\"Test set accuracy:\", result['test_score'])\n",
    "\n",
    "    # Classification Report & Confusion Matrix on test data\n",
    "    print(\"\\nClassification Report:\")\n",
    "    y_pred = result['best_model'].predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"===================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf210_gpu_env",
   "language": "python",
   "name": "tf210_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
